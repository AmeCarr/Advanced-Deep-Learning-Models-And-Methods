{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e184817",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:54.480176Z",
     "iopub.status.busy": "2023-01-09T22:06:54.479681Z",
     "iopub.status.idle": "2023-01-09T22:06:57.223535Z",
     "shell.execute_reply": "2023-01-09T22:06:57.222602Z"
    },
    "papermill": {
     "duration": 2.75921,
     "end_time": "2023-01-09T22:06:57.225983",
     "exception": false,
     "start_time": "2023-01-09T22:06:54.466773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install torchsummary\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import datetime\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "import csv\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, average_precision_score\n",
    "import torch.nn.functional as F\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a7f7b",
   "metadata": {
    "papermill": {
     "duration": 0.004984,
     "end_time": "2023-01-09T22:06:57.236453",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.231469",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "282333d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.248462Z",
     "iopub.status.busy": "2023-01-09T22:06:57.247997Z",
     "iopub.status.idle": "2023-01-09T22:06:57.276065Z",
     "shell.execute_reply": "2023-01-09T22:06:57.275177Z"
    },
    "papermill": {
     "duration": 0.036212,
     "end_time": "2023-01-09T22:06:57.277928",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.241716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dot_norm_exp(a,b):\n",
    "    dot = torch.sum(a * b, dim=1)\n",
    "    aa = torch.sum((a**2),dim=1)**0.5\n",
    "    bb = torch.sum((b**2),dim=1)**0.5\n",
    "    dot_norm = dot/(aa*bb)\n",
    "    ret = torch.exp(dot_norm)\n",
    "    return ret\n",
    "\n",
    "def dot_norm(a,b):\n",
    "    dot = torch.sum(a * b, dim=1)\n",
    "    aa = torch.sum((a**2),dim=1)**0.5\n",
    "    bb = torch.sum((b**2),dim=1)**0.5\n",
    "    dot_norm = dot/(aa*bb)\n",
    "    return dot_norm\n",
    "\n",
    "def dot(a,b):\n",
    "    dot = torch.sum(a * b, dim=1)\n",
    "    return dot\n",
    "\n",
    "def norm_euclidian(a,b):\n",
    "    aa = (torch.sum((a**2),dim=1)**0.5).unsqueeze(dim=1)\n",
    "    bb = (torch.sum((b**2),dim=1)**0.5).unsqueeze(dim=1)\n",
    "    return (torch.sum(((a/aa-b/bb)**2),dim=1)**0.5)\n",
    "\n",
    "def get_next_model_folder(prefix, path = ''):\n",
    "\n",
    "    model_folder = lambda prefix, run_idx: f\"{prefix}_model_run_{run_idx}\"\n",
    "\n",
    "    run_idx = 1\n",
    "    while os.path.isdir(os.path.join(path, model_folder(prefix, run_idx))):\n",
    "        run_idx += 1\n",
    "\n",
    "    model_path = os.path.join(path, model_folder(prefix, run_idx))\n",
    "    print(f\"STARTING {prefix} RUN {run_idx}! Storing the models at {model_path}\")\n",
    "\n",
    "    return model_path\n",
    "\n",
    "def get_random_patches(random_patch_loader, num_random_patches):\n",
    "\n",
    "        is_data_loader_finished = False\n",
    "\n",
    "        try:\n",
    "            img_batch = next(iter(random_patch_loader))['image']\n",
    "        except StopIteration:\n",
    "            is_data_loader_finished = True\n",
    "            # random_patch_loader = DataLoader(dataset_train, num_random_patches, shuffle=True)\n",
    "\n",
    "        if len(img_batch) < num_random_patches:\n",
    "            is_data_loader_finished = True\n",
    "\n",
    "        patches = []\n",
    "\n",
    "        for i in range(num_random_patches):\n",
    "            x = random.randint(0,6)\n",
    "            y = random.randint(0,6)\n",
    "\n",
    "            patches.append(img_batch[i:i+1,:,x*32:x*32+64,y*32:y*32+64])\n",
    "\n",
    "            # plt.imshow(np.transpose(patches[-1][0],(1,2,0)))\n",
    "            # plt.show()\n",
    "\n",
    "        patches_tensor = torch.cat(patches, dim=0)\n",
    "\n",
    "        return dict(\n",
    "            patches_tensor = patches_tensor,\n",
    "            is_data_loader_finished = is_data_loader_finished)\n",
    "\n",
    "# Tell how many parameters are on the model\n",
    "def inspect_model(model):\n",
    "    param_count = 0\n",
    "    for param_tensor_str in model.state_dict():\n",
    "        tensor_size = model.state_dict()[param_tensor_str].size()\n",
    "        print(f\"{param_tensor_str} size {tensor_size} = {model.state_dict()[param_tensor_str].numel()} params\")\n",
    "        param_count += model.state_dict()[param_tensor_str].numel()\n",
    "\n",
    "    print(f\"Number of parameters: {param_count}\")\n",
    "    \n",
    "def get_patch_tensor_from_image_batch(img_batch):\n",
    "\n",
    "    # Input of the function is a tensor [B, C, H, W]\n",
    "    # Output of the functions is a tensor [B * 49, C, 64, 64]\n",
    "\n",
    "    patch_batch = None\n",
    "    all_patches_list = []\n",
    "\n",
    "    for y_patch in range(7):\n",
    "        for x_patch in range(7):\n",
    "\n",
    "            y1 = y_patch * 32\n",
    "            y2 = y1 + 64\n",
    "\n",
    "            x1 = x_patch * 32\n",
    "            x2 = x1 + 64\n",
    "\n",
    "            img_patches = img_batch[:,:,y1:y2,x1:x2] # Batch(img_idx in batch), channels xrange, yrange\n",
    "            img_patches = img_patches.unsqueeze(dim=1)\n",
    "            all_patches_list.append(img_patches)\n",
    "\n",
    "            # print(patch_batch.shape)\n",
    "    all_patches_tensor = torch.cat(all_patches_list, dim=1)\n",
    "\n",
    "    patches_per_image = []\n",
    "    for b in range(all_patches_tensor.shape[0]):\n",
    "        patches_per_image.append(all_patches_tensor[b])\n",
    "\n",
    "    patch_batch = torch.cat(patches_per_image, dim = 0)\n",
    "    return patch_batch\n",
    "    \n",
    "def write_csv_stats(csv_path, stats_dict):\n",
    "\n",
    "    if not os.path.isfile(csv_path):\n",
    "        with open(csv_path, \"w\") as f:\n",
    "            csv_writer = csv.writer(f)\n",
    "            csv_writer.writerow(stats_dict.keys())\n",
    "\n",
    "    for key, value in stats_dict.items():\n",
    "        if isinstance(value, float):\n",
    "            precision = 0.001\n",
    "            stats_dict[key] =  ((value / precision ) // 1.0 ) * precision\n",
    "\n",
    "    with open(csv_path, \"a\") as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow(stats_dict.values())\n",
    "\n",
    "def compute_pre_recall_f1(target, pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(target, pred, average='binary')\n",
    "    return f1\n",
    "\n",
    "class EarlyStopper:\n",
    "\n",
    "    def stop(self, epoch, val_loss, val_auc=None,  test_loss=None, test_auc=None, test_ap=None,test_f1=None, train_loss=None,score=None,target=None):\n",
    "        raise NotImplementedError(\"Implement this method!\")\n",
    "\n",
    "    def get_best_vl_metrics(self):\n",
    "        return  self.train_loss, self.val_loss,self.val_auc,self.test_loss,self.test_auc,self.test_ap,self.test_f1, self.best_epoch,self.score,self.target\n",
    "\n",
    "class Patience(EarlyStopper):\n",
    "\n",
    "    '''\n",
    "    Implement common \"patience\" technique\n",
    "    '''\n",
    "\n",
    "    def __init__(self, patience=10, use_train_loss=True):\n",
    "        self.local_val_optimum = float(\"inf\")\n",
    "        self.use_train_loss = use_train_loss\n",
    "        self.patience = patience\n",
    "        self.best_epoch = -1\n",
    "        self.counter = -1\n",
    "\n",
    "        self.train_loss= None\n",
    "        self.val_loss, self.val_auc, = None, None\n",
    "        self.test_loss, self.test_auc,self.test_ap,self.test_f1 = None, None,None, None\n",
    "        self.score, self.target = None, None\n",
    "\n",
    "    def stop(self, epoch, val_loss, val_auc=None, test_loss=None, test_auc=None, test_ap=None,test_f1=None,train_loss=None,score=None,target=None):\n",
    "        if self.use_train_loss:\n",
    "            if train_loss <= self.local_val_optimum:\n",
    "                self.counter = 0\n",
    "                self.local_val_optimum = train_loss\n",
    "                self.best_epoch = epoch\n",
    "                self.train_loss= train_loss\n",
    "                self.val_loss, self.val_auc= val_loss, val_auc\n",
    "                self.test_loss, self.test_auc, self.test_ap,self.test_f1\\\n",
    "                    = test_loss, test_auc, test_ap,test_f1\n",
    "                self.score, self.target = score,target\n",
    "                return False\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                return self.counter >= self.patience\n",
    "        else:\n",
    "            if val_loss <= self.local_val_optimum:\n",
    "                self.counter = 0\n",
    "                self.local_val_optimum = val_loss\n",
    "                self.best_epoch = epoch\n",
    "                self.train_loss= train_loss\n",
    "                self.val_loss, self.val_auc = val_loss, val_auc\n",
    "                self.test_loss, self.test_auc, self.test_ap,self.test_f1\\\n",
    "                    = test_loss, test_auc, test_ap,test_f1\n",
    "                self.score, self.target = score, target\n",
    "                return False\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75178d2",
   "metadata": {
    "papermill": {
     "duration": 0.004683,
     "end_time": "2023-01-09T22:06:57.287508",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.282825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da8193d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.298769Z",
     "iopub.status.busy": "2023-01-09T22:06:57.298505Z",
     "iopub.status.idle": "2023-01-09T22:06:57.317144Z",
     "shell.execute_reply": "2023-01-09T22:06:57.316326Z"
    },
    "papermill": {
     "duration": 0.02681,
     "end_time": "2023-01-09T22:06:57.319216",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.292406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self, data_path, is_train, random_seed = 42, target_transform = None, num_classes = None):\n",
    "        super(ImageNetDataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "\n",
    "        self.is_classes_limited = False\n",
    "\n",
    "        if num_classes != None:\n",
    "            self.is_classes_limited = True\n",
    "            self.num_classes = num_classes\n",
    "\n",
    "        self.classes = []\n",
    "        class_idx = 0\n",
    "        for class_name in os.listdir(data_path):\n",
    "            if not os.path.isdir(os.path.join(data_path,class_name)):\n",
    "                continue\n",
    "            self.classes.append(\n",
    "               dict(\n",
    "                   class_idx = class_idx,\n",
    "                   class_name = class_name,\n",
    "               ))\n",
    "            class_idx += 1\n",
    "\n",
    "            if self.is_classes_limited:\n",
    "                if class_idx == self.num_classes:\n",
    "                    break\n",
    "\n",
    "        if not self.is_classes_limited:\n",
    "            self.num_classes = len(self.classes)\n",
    "\n",
    "        self.image_list = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(data_path, cls['class_name'])\n",
    "            for image_name in os.listdir(class_path):\n",
    "                image_path = os.path.join(class_path, image_name)\n",
    "                self.image_list.append(dict(\n",
    "                    cls = cls,\n",
    "                    image_path = image_path,\n",
    "                    image_name = image_name,\n",
    "                ))\n",
    "\n",
    "        self.img_idxes = np.arange(0,len(self.image_list))\n",
    "\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        if is_train:\n",
    "            np.random.shuffle(self.img_idxes)\n",
    "        last_train_sample = int(len(self.img_idxes))\n",
    "        self.img_idxes = self.img_idxes[:last_train_sample]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_idxes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img_idx = self.img_idxes[index]\n",
    "        img_info = self.image_list[img_idx]\n",
    "\n",
    "        img = Image.open(img_info['image_path'])\n",
    "        #print('IMG MODE: ' + str(img.mode))\n",
    "\n",
    "        if img.mode == 'L':\n",
    "            tr = transforms.Grayscale(num_output_channels=3)\n",
    "            img = tr(img)\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img1 = tr(img)\n",
    "\n",
    "        width, height = img.size\n",
    "        if min(width, height)>IMG_SIZE[0] * 1.5:\n",
    "            tr = transforms.Resize(int(IMG_SIZE[0] * 1.5))\n",
    "            img = tr(img)\n",
    "\n",
    "        width, height = img.size\n",
    "        if min(width, height)<IMG_SIZE[0]:\n",
    "            tr = transforms.Resize(IMG_SIZE)\n",
    "            img = tr(img)\n",
    "\n",
    "        tr = transforms.RandomCrop(IMG_SIZE)\n",
    "        img = tr(img)\n",
    "\n",
    "        tr = transforms.ToTensor()\n",
    "        img = tr(img)\n",
    "\n",
    "        if (img.shape[0] != 3):\n",
    "            img = img[0:3]\n",
    "            \n",
    "        #plt.imshow(img.permute(1,2,0))\n",
    "        #fig, axes = plt.subplots(7,7)\n",
    "\n",
    "        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n",
    "\n",
    "    def get_number_of_classes(self):\n",
    "        return self.num_classes\n",
    "\n",
    "    def get_number_of_samples(self):\n",
    "        return self.__len__()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        return [cls['class_name'] for cls in self.classes]\n",
    "\n",
    "    def get_class_name(self, class_idx):\n",
    "        return self.classes[class_idx]['class_name']\n",
    "\n",
    "\n",
    "def get_imagenet_datasets(train_path, test_path, train_split = 0.9, num_classes_train = None, num_classes_test = None, random_seed = None):\n",
    "\n",
    "    if random_seed == None:\n",
    "        random_seed = int(time.time())\n",
    "    dataset_train = ImageNetDataset(train_path, is_train = True, random_seed=random_seed, num_classes = num_classes_train)\n",
    "    trainset_size = int(len(dataset_train)*train_split)\n",
    "    validset_size = len(dataset_train) - trainset_size\n",
    "    dataset_train, dataset_valid = random_split(dataset_train, [trainset_size, validset_size])\n",
    "    dataset_test = ImageNetDataset(test_path, is_train = False, random_seed=random_seed, num_classes = num_classes_test)\n",
    "\n",
    "    return dataset_train, dataset_valid, dataset_test\n",
    "    \n",
    "def get_random_patch_loader(dataset_train):\n",
    "    return DataLoader(dataset_train, args.num_random_patches, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb1cc1",
   "metadata": {
    "papermill": {
     "duration": 0.004772,
     "end_time": "2023-01-09T22:06:57.329035",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.324263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Resnet Blocks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0882b",
   "metadata": {
    "papermill": {
     "duration": 0.004752,
     "end_time": "2023-01-09T22:06:57.338751",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.333999",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **ResNet18 block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97066b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.349805Z",
     "iopub.status.busy": "2023-01-09T22:06:57.349543Z",
     "iopub.status.idle": "2023-01-09T22:06:57.357226Z",
     "shell.execute_reply": "2023-01-09T22:06:57.356263Z"
    },
    "papermill": {
     "duration": 0.015515,
     "end_time": "2023-01-09T22:06:57.359185",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.343670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        if downsample:\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n",
    "                #nn.MaxPool2d(2,2);\n",
    "            )\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "            self.shortcut = nn.Sequential()\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
    "                               kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        shortcut = self.shortcut(input)\n",
    "        input = self.conv1(nn.ReLU()(self.bn1(input)))\n",
    "        input = self.conv2(nn.ReLU()(self.bn2(input)))\n",
    "        input = input + shortcut\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcae6ac",
   "metadata": {
    "papermill": {
     "duration": 0.004732,
     "end_time": "2023-01-09T22:06:57.369133",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.364401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **ResEncoderModel**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7ba195",
   "metadata": {
    "papermill": {
     "duration": 0.00472,
     "end_time": "2023-01-09T22:06:57.378829",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.374109",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **ResNet18 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbe44fe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.392188Z",
     "iopub.status.busy": "2023-01-09T22:06:57.390718Z",
     "iopub.status.idle": "2023-01-09T22:06:57.403099Z",
     "shell.execute_reply": "2023-01-09T22:06:57.402279Z"
    },
    "papermill": {
     "duration": 0.021132,
     "end_time": "2023-01-09T22:06:57.405093",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.383961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_channels, resblock, repeat, outputs=1024):\n",
    "        super().__init__()\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        filters = [64, 64, 128, 256] #, 512]\n",
    "\n",
    "        self.layer1 = nn.Sequential()\n",
    "        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n",
    "        for i in range(1, repeat[0]):\n",
    "                self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n",
    "\n",
    "        self.layer2 = nn.Sequential()\n",
    "        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n",
    "        for i in range(1, repeat[1]):\n",
    "                self.layer2.add_module('conv3_%d' % (\n",
    "                    i+1,), resblock(filters[2], filters[2], downsample=False))\n",
    "\n",
    "        self.layer3 = nn.Sequential()\n",
    "        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n",
    "        for i in range(1, repeat[2]):\n",
    "            self.layer3.add_module('conv2_%d' % (\n",
    "                i+1,), resblock(filters[3], filters[3], downsample=False))\n",
    "\n",
    "        #self.layer4 = nn.Sequential()\n",
    "        #self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n",
    "        #for i in range(1, repeat[3]):\n",
    "        #    self.layer4.add_module('conv3_%d'%(i+1,),resblock(filters[4], filters[4], downsample=False))\n",
    "\n",
    "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = torch.nn.Linear(filters[3], outputs)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.layer0(input)\n",
    "        input = self.layer1(input)\n",
    "        input = self.layer2(input)\n",
    "        input = self.layer3(input)\n",
    "        #input = self.layer4(input)\n",
    "        input = self.gap(input)\n",
    "        # torch.flatten()\n",
    "        # https://stackoverflow.com/questions/60115633/pytorch-flatten-doesnt-maintain-batch-size\n",
    "        input = torch.flatten(input, start_dim=1)\n",
    "        input = self.fc(input)\n",
    "\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90067d",
   "metadata": {
    "papermill": {
     "duration": 0.004705,
     "end_time": "2023-01-09T22:06:57.414711",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.410006",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DENSENET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6005ab0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.425942Z",
     "iopub.status.busy": "2023-01-09T22:06:57.425683Z",
     "iopub.status.idle": "2023-01-09T22:06:57.450741Z",
     "shell.execute_reply": "2023-01-09T22:06:57.449894Z"
    },
    "papermill": {
     "duration": 0.033079,
     "end_time": "2023-01-09T22:06:57.452741",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.419662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        return torch.cat([x, out], 1)\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, dropRate=0.0):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        inter_planes = out_planes * 4\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1,\n",
    "                               padding=0, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
    "        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "        return torch.cat([x, out], 1)\n",
    "\n",
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, dropRate=0.0):\n",
    "        super(TransitionBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n",
    "                               padding=0, bias=False)\n",
    "        self.droprate = dropRate\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n",
    "        return F.avg_pool2d(out, 2)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, growth_rate, block, dropRate=0.0):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, growth_rate, nb_layers, dropRate)\n",
    "    def _make_layer(self, block, in_planes, growth_rate, nb_layers, dropRate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_planes+i*growth_rate, growth_rate, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class DenseNet3(nn.Module):\n",
    "    def __init__(self, depth, growth_rate=12,\n",
    "                 reduction=0.5, bottleneck=True, dropRate=0.0):\n",
    "        super(DenseNet3, self).__init__()\n",
    "        in_planes = 2 * growth_rate\n",
    "        n = (depth - 4) / 3\n",
    "        if bottleneck == True:\n",
    "            n = n/2\n",
    "            block = BottleneckBlock\n",
    "        else:\n",
    "            block = BasicBlock\n",
    "        n = int(n)\n",
    "        # 1st conv before any dense block\n",
    "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
    "        in_planes = int(in_planes+n*growth_rate)\n",
    "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        # 2nd block\n",
    "        self.block2 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
    "        in_planes = int(in_planes+n*growth_rate)\n",
    "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        # 3rd block\n",
    "        self.block3 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n",
    "        in_planes = int(in_planes+n*growth_rate)\n",
    "        self.conv2 = nn.Conv2d(in_channels=342, out_channels=256, kernel_size=3)\n",
    "        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = torch.nn.Linear(256, 1024)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        #print(\"CONV1: \", out.shape)\n",
    "\n",
    "        out = self.trans1(self.block1(out))\n",
    "        #print(\"TRANSF1: \", out.shape)\n",
    "\n",
    "        out = self.trans2(self.block2(out))\n",
    "        #print(\"BLOCK2: \", out.shape)\n",
    "\n",
    "        out = self.block3(out)\n",
    "        #print(\"BLOCK3: \", out.shape)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        #print(\"CONV2D: \", out.shape)\n",
    "        \n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        #print(\"avg_pool2d: \", out.shape)\n",
    "\n",
    "        out = self.gap(out)\n",
    "        #print(\"GAP: \", out.shape)\n",
    "\n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        #print(\"FLATTEN: \", out.shape)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        #print(\"FC: \", out.shape)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162bf816",
   "metadata": {
    "papermill": {
     "duration": 0.004749,
     "end_time": "2023-01-09T22:06:57.462422",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.457673",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Context Prediction Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38f7b06",
   "metadata": {
    "papermill": {
     "duration": 0.004671,
     "end_time": "2023-01-09T22:06:57.472132",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.467461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **CPC with only encoder and predicts the underlying patches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e797cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.483139Z",
     "iopub.status.busy": "2023-01-09T22:06:57.482880Z",
     "iopub.status.idle": "2023-01-09T22:06:57.497486Z",
     "shell.execute_reply": "2023-01-09T22:06:57.496614Z"
    },
    "papermill": {
     "duration": 0.022296,
     "end_time": "2023-01-09T22:06:57.499307",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.477011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ContextPredictionModelWithDir(Module):\n",
    "\n",
    "    def __init__(self, in_channels, direction):\n",
    "        super(ContextPredictionModelWithDir, self).__init__()\n",
    "        # Input will be 1024x7x7\n",
    "        self.in_channels = in_channels\n",
    "        self.direction = direction\n",
    "        \n",
    "        self.prediction_weights = nn.ModuleList([nn.Linear(\n",
    "                        in_features = self.in_channels,\n",
    "                        out_features = self.in_channels,\n",
    "                    ) for i in range(4)])\n",
    "        \n",
    "\n",
    "    # x: encoded patches (2, 1024, 7, 7)\n",
    "    def forward(self, x): \n",
    "\n",
    "        z_patches_loc_list = []\n",
    "        context_vectors_list = []\n",
    "\n",
    "        if self.direction == 'DOWN' or self.direction == 'UP':\n",
    "            for y1 in range(3): #rows\n",
    "                z_patches_list = []\n",
    "                for x1 in range(7): #columns\n",
    "                    if self.direction == 'DOWN':\n",
    "                        z_patches = x[:, :, y1:y1+1, x1:x1+1] #2, 1024, 1, 1\n",
    "                        z_patches_loc = (y1,x1) # Store pixel coordinates\n",
    "                    else:\n",
    "                        z_patches = x[:, :, y1+4:y1+5, x1:x1+1] #2, 1024, 1, 1\n",
    "                        z_patches_loc = (y1+4,x1) # Store pixel coordinates\n",
    "                    #print('z_patches: ' + str(z_patches.size()))\n",
    "                    z_patches_list.append(z_patches) \n",
    "                    #print('List: ' + str(len(z_patches_list)))\n",
    "                    z_patches_loc_list += [z_patches_loc] * len(z_patches)\n",
    "                    #print('Loc List: ' + str(len(z_patches_loc_list)))\n",
    "\n",
    "                z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, 1, 1\n",
    "                #print('Tensor: '+ str(y1) + str(z_patches_tensor.size()))\n",
    "                z_patches_list.append(z_patches_tensor) # 3\n",
    "        else:\n",
    "            for y1 in range(7): #rows\n",
    "                z_patches_list = []\n",
    "                for x1 in range(3): #columns\n",
    "                    if self.direction == 'RIGHT':\n",
    "                        z_patches = x[:, :, y1:y1+1, x1:x1+1] #2, 1024, 1, 1\n",
    "                        z_patches_loc = (y1,x1) # Store pixel coordinates\n",
    "                    else:\n",
    "                        z_patches = x[:, :, y1:y1+1, x1+4:x1+5] #2, 1024, 1, 1\n",
    "                        z_patches_loc = (y1,x1+4) # Store pixel coordinates\n",
    "                    #print('z_patches: ' + str(z_patches.size()))\n",
    "                    z_patches_list.append(z_patches) \n",
    "                    #print('List: ' + str(len(z_patches_list)))\n",
    "                    z_patches_loc_list += [z_patches_loc] * len(z_patches)\n",
    "                    #print('Loc List: ' + str(len(z_patches_loc_list)))\n",
    "\n",
    "                z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, 1, 1\n",
    "                #print('Tensor: '+ str(y1) + str(z_patches_tensor.size()))\n",
    "                z_patches_list.append(z_patches_tensor) # 3\n",
    "            \n",
    "        z_patches = torch.cat(z_patches_list, dim = 0) #42, 1024, 1, 1\n",
    "        #print('Z_patches_vector: ' + str(z_patches.size()))\n",
    "        z_patches = z_patches.squeeze(dim=3)\n",
    "        z_patches = z_patches.squeeze(dim=2)\n",
    "\n",
    "        context_loc_list = torch.tensor(z_patches_loc_list)\n",
    "\n",
    "        all_predictions = []\n",
    "        all_loc = []\n",
    "\n",
    "        for steps_y in range(4):\n",
    "            predictions = self.prediction_weights[steps_y].forward(z_patches) #42, 1024\n",
    "            #print('Predictions: ' + str(predictions.size()))\n",
    "            all_predictions.append(predictions)\n",
    "            if self.direction == 'DOWN':\n",
    "                steps_add = torch.tensor([steps_y + 1, 0])\n",
    "            elif self.direction == 'UP':\n",
    "                steps_add = torch.tensor([0 - 1 - steps_y, 0])\n",
    "            elif self.direction == 'RIGHT':\n",
    "                steps_add = torch.tensor([0, steps_y + 1])\n",
    "            else:\n",
    "                steps_add = torch.tensor([0, 0 - 1 - steps_y])\n",
    "            all_loc.append(context_loc_list + steps_add)\n",
    "\n",
    "        ret = torch.cat(all_predictions, dim = 0), torch.cat(all_loc, dim = 0)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf7315c",
   "metadata": {
    "papermill": {
     "duration": 0.004742,
     "end_time": "2023-01-09T22:06:57.508885",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.504143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Context predictor training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9152670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.520213Z",
     "iopub.status.busy": "2023-01-09T22:06:57.519734Z",
     "iopub.status.idle": "2023-01-09T22:06:57.545695Z",
     "shell.execute_reply": "2023-01-09T22:06:57.544900Z"
    },
    "papermill": {
     "duration": 0.033876,
     "end_time": "2023-01-09T22:06:57.547641",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.513765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_validation(args, res_encoder_model, context_predictor_model, random_patch_loader, data_loader_train, data_loader_valid):\n",
    "    res_encoder_model.eval()\n",
    "    context_predictor_model.eval()\n",
    "    \n",
    "    loss_total = 0\n",
    "    for i, data in enumerate(data_loader_valid):\n",
    "        img = data['image'].to(args.device)\n",
    "        patches = get_patch_tensor_from_image_batch(img)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n",
    "            if patches_return['is_data_loader_finished']:\n",
    "                random_patch_loader = get_random_patch_loader(dataset_train)\n",
    "            else:\n",
    "                random_patches = patches_return['patches_tensor'].to(args.device)\n",
    "            \n",
    "            enc_patches = res_encoder_model(patches)\n",
    "            enc_patches = enc_patches.view(1,7,7,-1)\n",
    "            enc_patches = enc_patches.permute(0,3,1,2)\n",
    "            \n",
    "            predictions, locations = context_predictor_model(enc_patches)\n",
    "            \n",
    "            enc_random_patches = res_encoder_model(random_patches)\n",
    "            \n",
    "            loss = calculate_score_dir(enc_patches, predictions, locations, enc_random_patches)\n",
    "            loss_total += loss\n",
    "            \n",
    "    return loss_total / len(data_loader_valid)         \n",
    "\n",
    "def run_context_predictor(args, denseNet_model, context_predictor_model, models_store_path):\n",
    "\n",
    "    print(\"RUNNING CONTEXT PREDICTOR \" +str(context_predictor_model.direction)+ \" TRAINING\")\n",
    "    \n",
    "    #used to create the file where model weights are saved\n",
    "    prefix = str(context_predictor_model.direction)\n",
    "    best_encoder = lambda prefix: f\"{prefix}_best_res_encoder_weights.pt\"\n",
    "    best_context = lambda prefix: f\"{prefix}_best_context_weights.pt\"\n",
    "    \n",
    "    #upload of datasets\n",
    "    dataset_train, dataset_valid, dataset_test = get_imagenet_datasets(args.train_image_folder, args.test_image_folder, num_classes_train = args.num_classes_train, num_classes_test = args.num_classes_test)\n",
    "    \n",
    "    stats_csv_path = os.path.join(models_store_path, \"pred_stats.csv\")\n",
    "\n",
    "    #creation of dataloaders\n",
    "    random_patch_loader = get_random_patch_loader(dataset_train)\n",
    "    data_loader_train = DataLoader(dataset_train, args.sub_batch_size, shuffle = True)\n",
    "    data_loader_valid = DataLoader(dataset_valid, 1, shuffle = True)\n",
    "\n",
    "    params = list(denseNet_model.parameters()) + list(context_predictor_model.parameters())\n",
    "    optimizer = torch.optim.Adam(params = params, lr=0.00015)\n",
    "    #early_stopper = Patience(20, True)\n",
    "\n",
    "    trigger = 0\n",
    "    patience = args.patience\n",
    "    sub_batches_processed = 0\n",
    "    batch_loss = 0\n",
    "    sum_batch_loss = 0 \n",
    "    best_batch_loss = 1e10\n",
    "    best_valid_loss = 1e10\n",
    "\n",
    "    z_vect_similarity = dict()\n",
    "    \n",
    "    for epoch in range(1, args.num_epochs + 1):\n",
    "        \n",
    "        print(\"RUNNING EPOCH #\" + str(epoch))\n",
    "        denseNet_model.train()\n",
    "        context_predictor_model.train()\n",
    "        \n",
    "        for batch in data_loader_train:\n",
    "\n",
    "            img_batch = batch['image'].to(args.device)\n",
    "            patch_batch = get_patch_tensor_from_image_batch(img_batch)\n",
    "            #print('Patch_batch: ' + str(patch_batch.size()))\n",
    "            batch_size = len(img_batch)\n",
    "\n",
    "            # Apply encoder to all the 49 patches of the image (64x64)\n",
    "            patches_encoded = denseNet_model.forward(patch_batch) #98, 1024\n",
    "            #print(\"PATCHES_ENCODED SIZE AFTER RES_ENCODER_MODEL\", patches_encoded.shape)\n",
    "            #print('Patch_encoded: ' + str(patches_encoded.size()))\n",
    "            patches_encoded = patches_encoded.view(batch_size, 7,7,-1) #reshape 2, 7, 7, 1024\n",
    "            #print('Reshape: ' + str(patches_encoded.size()))\n",
    "            patches_encoded = patches_encoded.permute(0,3,1,2) #2, 1024, 7, 7\n",
    "            #print('Permute: ' + str(patches_encoded.size()))\n",
    "\n",
    "            for i in range(2):\n",
    "                patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n",
    "                if patches_return['is_data_loader_finished']:\n",
    "                    random_patch_loader = get_random_patch_loader(dataset_train)\n",
    "                else:\n",
    "                    random_patches = patches_return['patches_tensor'].to(args.device)\n",
    "\n",
    "            # enc_random_patches = resnet_encoder.forward(random_patches).detach()\n",
    "            # Apply encoder to few rendom patches\n",
    "            enc_random_patches = denseNet_model.forward(random_patches)\n",
    "\n",
    "            # Apply context_predictor to encoded patches\n",
    "            predictions, locations = context_predictor_model.forward(patches_encoded) #112, 1024\n",
    "            #print('Predictions: ' + str(predictions.size())) \n",
    "            losses = []\n",
    "\n",
    "            for b in range(len(predictions)//batch_size): #batch_size = 2\n",
    "\n",
    "                b_idx_start = b*batch_size\n",
    "                b_idx_end = (b+1)*batch_size\n",
    "\n",
    "                p_y = locations[b_idx_start][0]\n",
    "                p_x = locations[b_idx_start][1]\n",
    "\n",
    "                # Encoded patches on the same position of the predictions (Z_i+k,j)\n",
    "                target = patches_encoded[:,:,p_y,p_x]\n",
    "                # Predicted patches done by context predictor (Zcap_i+k,j = W_k * c_i,j)\n",
    "                pred = predictions[b_idx_start:b_idx_end] #2,1024\n",
    "\n",
    "                #dot_norm_val = dot_norm_exp(pred.detach().to('cpu'), target.detach().to('cpu'))\n",
    "                #euc_loss_val = norm_euclidian(pred.detach().to('cpu'), target.detach().to('cpu'))\n",
    "\n",
    "                # Mul between predictions and encoded patches (Zcap_i+k,j * Z_i+k,j)\n",
    "                good_term_dot = dot(pred, target) #dot_norm_val #dot(pred, target)\n",
    "                dot_terms = [torch.unsqueeze(good_term_dot,dim=0)]\n",
    "\n",
    "                for random_patch_idx in range(args.num_random_patches):\n",
    "                    # Mul between predictions and ancoded random patches (Zcap_i+k,j * Z_l)\n",
    "                    #bad_term_dot = dot_norm_exp(pred.detach().to('cpu'), enc_random_patches[random_patch_idx:random_patch_idx+1].detach().to('cpu'))\n",
    "                    bad_term_dot = dot(pred, enc_random_patches[random_patch_idx:random_patch_idx+1])\n",
    "                    dot_terms.append(torch.unsqueeze(bad_term_dot, dim=0))\n",
    "\n",
    "                log_softmax = torch.log_softmax(torch.cat(dot_terms, dim=0), dim=0)\n",
    "                losses.append(-log_softmax[0,])\n",
    "\n",
    "            loss = torch.mean(torch.cat(losses))\n",
    "            loss.backward()\n",
    "\n",
    "            sub_batches_processed += img_batch.shape[0]\n",
    "            batch_loss += loss.detach().to('cpu')\n",
    "            sum_batch_loss += torch.sum(torch.cat(losses).detach().to('cpu'))\n",
    "\n",
    "            if sub_batches_processed >= args.batch_size:\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                print(f\"{datetime.datetime.now()} Loss: {batch_loss}\")\n",
    "                print(f\"{datetime.datetime.now()} SUM Loss: {sum_batch_loss}\")\n",
    "\n",
    "                torch.save(denseNet_model.state_dict(), os.path.join(models_store_path, \"last_res_encoder_weights.pt\"))\n",
    "                torch.save(context_predictor_model.state_dict(), os.path.join(models_store_path, \"last_context_predictor_weights.pt\"))\n",
    "                \n",
    "                if best_batch_loss > batch_loss:\n",
    "                    best_batch_loss = batch_loss\n",
    "                    best_encoder_model = denseNet_model #.state_dict()\n",
    "                    best_context_model = context_predictor_model #.state_dict()\n",
    "                    torch.save(denseNet_model.state_dict(), os.path.join(models_store_path, best_encoder(prefix)))\n",
    "                    torch.save(context_predictor_model.state_dict(), os.path.join(models_store_path, best_context(prefix)))\n",
    "\n",
    "                for key, cos_similarity_tensor in z_vect_similarity.items():\n",
    "                    print(f\"Mean cos_sim for class {key} is {cos_similarity_tensor.mean()} . Number: {cos_similarity_tensor.size()}\")\n",
    "\n",
    "                z_vect_similarity = dict()\n",
    "\n",
    "                stats = dict(\n",
    "                    batch_loss = batch_loss,\n",
    "                    sum_batch_loss = sum_batch_loss\n",
    "                )\n",
    "                write_csv_stats(stats_csv_path, stats)\n",
    "\n",
    "                sub_batches_processed = 0\n",
    "                batch_loss = 0\n",
    "                sum_batch_loss = 0\n",
    "                \n",
    "        # Early stopping\n",
    "        if epoch % 5 == 0:\n",
    "            valid_loss = run_validation(args, best_encoder_model, best_context_model, random_patch_loader, data_loader_train, data_loader_valid)\n",
    "            print('Validation Loss:' +str(valid_loss))\n",
    "            \n",
    "            if valid_loss > best_valid_loss:\n",
    "                trigger += 1\n",
    "                if trigger >= patience:\n",
    "                    n = epoch - patience\n",
    "                    print('Early Stopping! Find best epoch: ' + str(n))\n",
    "                    torch.save(real_best_encoder.state_dict(), os.path.join(models_store_path, best_encoder(prefix)))\n",
    "                    torch.save(real_best_context.state_dict(), os.path.join(models_store_path, best_context(prefix)))\n",
    "                    return\n",
    "            else:\n",
    "                trigger = 0\n",
    "                real_best_encoder = best_encoder_model\n",
    "                real_best_context = best_context_model\n",
    "                best_valid_loss = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eecf957",
   "metadata": {
    "papermill": {
     "duration": 0.004734,
     "end_time": "2023-01-09T22:06:57.557566",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.552832",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Anomaly detection evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "444c6647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.569113Z",
     "iopub.status.busy": "2023-01-09T22:06:57.568853Z",
     "iopub.status.idle": "2023-01-09T22:06:57.591105Z",
     "shell.execute_reply": "2023-01-09T22:06:57.590213Z"
    },
    "papermill": {
     "duration": 0.030727,
     "end_time": "2023-01-09T22:06:57.593271",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.562544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_score_dir(enc_patches, predictions, locations, enc_random_patches):\n",
    "    losses = []\n",
    "\n",
    "    for b in range(len(predictions)):\n",
    "        \n",
    "        p_y = locations[b][0]\n",
    "        p_x = locations[b][1]\n",
    "\n",
    "        target = enc_patches[:,:,p_y,p_x]\n",
    "        pred = predictions[b]\n",
    "\n",
    "        #dot_norm_val = dot_norm_exp(x_tk.detach().to(args.device), x_t.detach().to(args.device))\n",
    "        #euc_loss_val = norm_euclidian(x_tk.detach().to(args.device), x_t.detach().to(args.device))\n",
    "\n",
    "        good_term_dot = dot(pred, target)#dot_norm_val #dot(pred, target)\n",
    "        dot_terms = [torch.unsqueeze(good_term_dot,dim=0)]\n",
    "\n",
    "        for random_patch_idx in range(args.num_random_patches):\n",
    "            #bad_term_dot = dot_norm_exp(pred.detach().to('cpu'), enc_random_patches[random_patch_idx:random_patch_idx+1].detach().to('cpu'))\n",
    "            bad_term_dot = dot(pred, enc_random_patches[random_patch_idx:random_patch_idx+1])\n",
    "            dot_terms.append(torch.unsqueeze(bad_term_dot, dim=0))\n",
    "\n",
    "        log_softmax = torch.log_softmax(torch.cat(dot_terms, dim=0), dim=0)\n",
    "        losses.append(-log_softmax[0,])\n",
    "        # losses.append(-torch.log(good_term/divisor))\n",
    "\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    return loss\n",
    "\n",
    "def run_anomaly_evaluation(args, res_encoder_model_list, context_model_list, models_store_path):\n",
    "\n",
    "    print(\"RUNNING ANOMALY DETECTION\")\n",
    "\n",
    "    dataset_train, dataset_valid, dataset_test = get_imagenet_datasets(args.train_image_folder, args.test_image_folder, train_split=1, num_classes_train = args.num_classes_train, num_classes_test = args.num_classes_test)\n",
    "    data_loader_test = DataLoader(dataset_test, 1, shuffle = False)\n",
    "    NUM_TEST_SAMPLES = dataset_test.get_number_of_samples()\n",
    "    print(NUM_TEST_SAMPLES)\n",
    "\n",
    "    random_patch_loader = get_random_patch_loader(dataset_train) \n",
    "    \n",
    "    for i, res in enumerate(res_encoder_model_list):\n",
    "        res.eval()\n",
    "        context_model_list[i].eval()\n",
    "    \n",
    "    score_all = []\n",
    "    label_all = []\n",
    "    data_list = []\n",
    "    \n",
    "    for i, data in enumerate(data_loader_test):\n",
    "        data_list.append(data)\n",
    "        class_name = data['class_name'][0]\n",
    "        #print('Class: ' +str(class_name))\n",
    "        \n",
    "        img = data['image'].to(args.device)\n",
    "        patches = get_patch_tensor_from_image_batch(img)\n",
    "        \n",
    "        image_scores = []\n",
    "        enc_patches = []\n",
    "        enc_random_patches = []\n",
    "        pred_list = []\n",
    "        location_list = []\n",
    "        with torch.no_grad():\n",
    "            # Get random patches from images not anomalous\n",
    "            patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n",
    "            if patches_return['is_data_loader_finished']:\n",
    "                random_patch_loader = get_random_patch_loader(dataset_train)\n",
    "            else:\n",
    "                random_patches = patches_return['patches_tensor'].to(args.device)\n",
    "                \n",
    "            for r, res in enumerate(res_encoder_model_list):\n",
    "                # Encode patches of test image\n",
    "                temp_patches = res(patches) #49, 1024\n",
    "                temp_patches = temp_patches.view(1,7,7,-1) #reshape 1, 7, 7, 1024\n",
    "                temp_patches = temp_patches.permute(0,3,1,2) #1, 1024, 7, 7\n",
    "                enc_patches.append(temp_patches)\n",
    "                \n",
    "                # Encode patches of random images\n",
    "                temp_random_patches = res(random_patches) #49, 1024\n",
    "                enc_random_patches.append(temp_random_patches)\n",
    "                \n",
    "                # Predictions from encoded patches\n",
    "                temp_pred, temp_locations = context_model_list[r](temp_patches) #112, 1024\n",
    "                pred_list.append(temp_pred)\n",
    "                location_list.append(temp_locations)    \n",
    "           \n",
    "            print('Image #'+str(i))\n",
    "            for j, enc_p in enumerate(enc_patches):\n",
    "                score = calculate_score_dir(enc_p, pred_list[j], location_list[j], enc_random_patches[j])\n",
    "                print('Score ' +str(j)+ ': ' + str(score.item()))\n",
    "                image_scores.append(score)\n",
    "            \n",
    "            if class_name == 'good':\n",
    "                label_all.append(0)\n",
    "            else:\n",
    "                label_all.append(1)\n",
    "            \n",
    "            avg_score = sum(image_scores) / len(image_scores) #Media delle loss dei 4 modelli\n",
    "            #max_score = torch.max(torch.cat(image_scores)) Loss massima tra i 4 modelli\n",
    "            score_all.append(avg_score)\n",
    "\n",
    "    \n",
    "    score_all = [s.cpu().numpy() for s in score_all]\n",
    "    score_all = np.vstack(score_all)\n",
    "    score_all = np.concatenate(score_all)\n",
    "    \n",
    "    # Compute threshold -> predictions\n",
    "    normal_ratio = sum(1 for a in label_all if a == 0) / len(label_all)\n",
    "    threshold = np.percentile(score_all, 100 * normal_ratio)\n",
    "    predictions = np.zeros(len(score_all))\n",
    "    predictions[score_all > threshold] = 1\n",
    "    \n",
    "    with open('{0}/Grid.csv'.format(models_store_path), mode='w') as csv_file:\n",
    "        fieldnames = ['Class_name', 'Score', 'Anomaly', 'AUC', 'F1', 'Average precision']\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for i, data in enumerate(data_list):\n",
    "            print(\"IMAGE #\" + str(i))\n",
    "            print(\"Class name: \" + str(data['class_name'][0]))\n",
    "            print(\"Average score: \" + str(score_all[i]))\n",
    "            print(\"Predictions: \" + str(predictions[i]))\n",
    "            \n",
    "            writer.writerow({\n",
    "                'Class_name': data['class_name'][0],\n",
    "                'Score': score_all[i],\n",
    "                'Anomaly': predictions[i]})\n",
    "    \n",
    "        auc = roc_auc_score(label_all, score_all)\n",
    "        f1 = compute_pre_recall_f1(label_all, predictions)\n",
    "        ap = average_precision_score(label_all, score_all)\n",
    "        print('AUC: ' + str(auc) + '\\nF1: ' + str(f1) + '\\nAverage Precision: ' + str(ap))\n",
    "        writer.writerow({\n",
    "            'AUC': auc,\n",
    "            'F1': f1,\n",
    "            'Average precision': ap})             \n",
    "        \n",
    " #       import torchvision.transforms as T\n",
    " #       from PIL import Image\n",
    "        # define a transform to convert a tensor to PIL image\n",
    " #       transform = T.ToPILImage()\n",
    "        # convert the tensor to PIL image using above transform\n",
    " #       img = img.permute(1,2,3,0)\n",
    " #       img = img.view(3, 256, 256)\n",
    " #       img = transform(img)\n",
    " #       plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a771a",
   "metadata": {
    "papermill": {
     "duration": 0.005109,
     "end_time": "2023-01-09T22:06:57.603285",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.598176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **MAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a98e6b",
   "metadata": {
    "papermill": {
     "duration": 0.004778,
     "end_time": "2023-01-09T22:06:57.613002",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.608224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **Contrastive Predictive Coding MAIN**\n",
    "> Main for training the CPC composed by encoder and autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2f650",
   "metadata": {
    "papermill": {
     "duration": 0.004898,
     "end_time": "2023-01-09T22:06:57.623112",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.618214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* **Anomaly Detection MAIN**\n",
    "> Main for detecting anomalies in pictures using Contrastive Predictive Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6706019e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-09T22:06:57.634275Z",
     "iopub.status.busy": "2023-01-09T22:06:57.633982Z",
     "iopub.status.idle": "2023-01-09T22:08:41.117250Z",
     "shell.execute_reply": "2023-01-09T22:08:41.114338Z"
    },
    "papermill": {
     "duration": 103.492119,
     "end_time": "2023-01-09T22:08:41.120209",
     "exception": false,
     "start_time": "2023-01-09T22:06:57.628090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CPC with args Namespace(batch_size=16, bottleneck=True, device='cuda', droprate=0, growth=12, layers=100, mode='anomaly_evaluation', num_classes_test=4, num_classes_train=1, num_epochs=30, num_random_patches=15, patience=10, reduce=0.5, sub_batch_size=2, test_image_folder='../input/mvtec-ad/bottle/test', train_image_folder='../input/mvtec-ad/grid/train')\n",
      "Loading res encoder DOWN weights from /kaggle/input/outputtrain/DOWN_best_res_encoder_weights.pt\n",
      "Loading context DOWN weights from /kaggle/input/outputtrain/DOWN_best_context_weights.pt\n",
      "Loading res encoder UP weights from /kaggle/input/outputtrain/UP_best_res_encoder_weights.pt\n",
      "Loading context UP weights from /kaggle/input/outputtrain/UP_best_context_weights.pt\n",
      "Loading res encoder RIGHT weights from /kaggle/input/outputtrain/RIGHT_best_res_encoder_weights.pt\n",
      "Loading context RIGHT weights from /kaggle/input/outputtrain/RIGHT_best_context_weights.pt\n",
      "Loading res encoder LEFT weights from /kaggle/input/outputtrain/LEFT_best_res_encoder_weights.pt\n",
      "Loading context LEFT weights from /kaggle/input/outputtrain/LEFT_best_context_weights.pt\n",
      "RUNNING ANOMALY DETECTION\n",
      "83\n",
      "Image #0\n",
      "Score 0: 0.0005000579403713346\n",
      "Score 1: 0.06104070320725441\n",
      "Score 2: 0.26684820652008057\n",
      "Score 3: 0.09674393385648727\n",
      "Image #1\n",
      "Score 0: 8.899254316929728e-05\n",
      "Score 1: 0.0912410169839859\n",
      "Score 2: 0.004449252504855394\n",
      "Score 3: 0.06523016095161438\n",
      "Image #2\n",
      "Score 0: 0.13452433049678802\n",
      "Score 1: 0.29474323987960815\n",
      "Score 2: 1.337332010269165\n",
      "Score 3: 0.5759985446929932\n",
      "Image #3\n",
      "Score 0: 0.026360569521784782\n",
      "Score 1: 0.12991595268249512\n",
      "Score 2: 0.07981154322624207\n",
      "Score 3: 0.2613660395145416\n",
      "Image #4\n",
      "Score 0: 6.982136824262852e-07\n",
      "Score 1: 0.06507687270641327\n",
      "Score 2: 0.0318751186132431\n",
      "Score 3: 0.01973465085029602\n",
      "Image #5\n",
      "Score 0: 8.967989560915157e-05\n",
      "Score 1: 0.06046314164996147\n",
      "Score 2: 0.8961199522018433\n",
      "Score 3: 0.0032371548004448414\n",
      "Image #6\n",
      "Score 0: 1.0260406497764052e-06\n",
      "Score 1: 0.0006019554566591978\n",
      "Score 2: 0.0013422531774267554\n",
      "Score 3: 0.0009266079287044704\n",
      "Image #7\n",
      "Score 0: 1.2015200809400994e-05\n",
      "Score 1: 0.08822418749332428\n",
      "Score 2: 0.0448945090174675\n",
      "Score 3: 0.013454797677695751\n",
      "Image #8\n",
      "Score 0: 2.22569487959845e-05\n",
      "Score 1: 0.006065454799681902\n",
      "Score 2: 0.0046683489345014095\n",
      "Score 3: 0.2772696912288666\n",
      "Image #9\n",
      "Score 0: 0.007204581517726183\n",
      "Score 1: 0.06340749561786652\n",
      "Score 2: 0.6427690386772156\n",
      "Score 3: 0.0802745521068573\n",
      "Image #10\n",
      "Score 0: 6.4713384517745e-07\n",
      "Score 1: 0.029189739376306534\n",
      "Score 2: 0.00018872504006139934\n",
      "Score 3: 0.08093130588531494\n",
      "Image #11\n",
      "Score 0: 8.418267680099234e-05\n",
      "Score 1: 0.6551409959793091\n",
      "Score 2: 0.12894286215305328\n",
      "Score 3: 0.3812394142150879\n",
      "Image #12\n",
      "Score 0: 0.0016366407508030534\n",
      "Score 1: 0.056436169892549515\n",
      "Score 2: 0.3451973795890808\n",
      "Score 3: 0.11908888816833496\n",
      "Image #13\n",
      "Score 0: 4.868020914727822e-05\n",
      "Score 1: 0.22840516269207\n",
      "Score 2: 3.793082578340545e-05\n",
      "Score 3: 0.18119071424007416\n",
      "Image #14\n",
      "Score 0: 0.04277897998690605\n",
      "Score 1: 0.07185667008161545\n",
      "Score 2: 0.10395433008670807\n",
      "Score 3: 0.09842629730701447\n",
      "Image #15\n",
      "Score 0: 0.3700268566608429\n",
      "Score 1: 0.15834489464759827\n",
      "Score 2: 0.7594732642173767\n",
      "Score 3: 0.09791862964630127\n",
      "Image #16\n",
      "Score 0: 6.949680482648546e-06\n",
      "Score 1: 0.0459115169942379\n",
      "Score 2: 0.5538336038589478\n",
      "Score 3: 0.0001965221599675715\n",
      "Image #17\n",
      "Score 0: 1.3206780749897007e-05\n",
      "Score 1: 0.2953169345855713\n",
      "Score 2: 0.12488063424825668\n",
      "Score 3: 0.06324107944965363\n",
      "Image #18\n",
      "Score 0: 0.43860843777656555\n",
      "Score 1: 0.1295289844274521\n",
      "Score 2: 0.9682638645172119\n",
      "Score 3: 0.905341625213623\n",
      "Image #19\n",
      "Score 0: 0.00027213539578951895\n",
      "Score 1: 0.37128883600234985\n",
      "Score 2: 0.05691501498222351\n",
      "Score 3: 0.7043728232383728\n",
      "Image #20\n",
      "Score 0: 0.055623553693294525\n",
      "Score 1: 0.7312161922454834\n",
      "Score 2: 3.854827880859375\n",
      "Score 3: 3.15008544921875\n",
      "Image #21\n",
      "Score 0: 0.02649773471057415\n",
      "Score 1: 0.17246867716312408\n",
      "Score 2: 0.41421908140182495\n",
      "Score 3: 3.3281655311584473\n",
      "Image #22\n",
      "Score 0: 0.37103745341300964\n",
      "Score 1: 0.33711808919906616\n",
      "Score 2: 0.08562850952148438\n",
      "Score 3: 3.302147388458252\n",
      "Image #23\n",
      "Score 0: 0.004409948363900185\n",
      "Score 1: 1.4324288368225098\n",
      "Score 2: 0.006283571477979422\n",
      "Score 3: 2.484254837036133\n",
      "Image #24\n",
      "Score 0: 1.277242311914506e-08\n",
      "Score 1: 0.14749403297901154\n",
      "Score 2: 0.00246227509342134\n",
      "Score 3: 0.026919126510620117\n",
      "Image #25\n",
      "Score 0: 0.4986427426338196\n",
      "Score 1: 0.8539707064628601\n",
      "Score 2: 2.069026470184326\n",
      "Score 3: 3.137777090072632\n",
      "Image #26\n",
      "Score 0: 11.165038108825684\n",
      "Score 1: 21.92040252685547\n",
      "Score 2: 4.2291646003723145\n",
      "Score 3: 1.654911756515503\n",
      "Image #27\n",
      "Score 0: 1.28765869140625\n",
      "Score 1: 1.1777081489562988\n",
      "Score 2: 0.0032658688724040985\n",
      "Score 3: 2.471886157989502\n",
      "Image #28\n",
      "Score 0: 1.4828636646270752\n",
      "Score 1: 4.105706214904785\n",
      "Score 2: 0.38059717416763306\n",
      "Score 3: 0.097406767308712\n",
      "Image #29\n",
      "Score 0: 0.0027104937471449375\n",
      "Score 1: 3.2022080421447754\n",
      "Score 2: 0.07163553684949875\n",
      "Score 3: 10.661808967590332\n",
      "Image #30\n",
      "Score 0: 0.007872562855482101\n",
      "Score 1: 0.6931943297386169\n",
      "Score 2: 1.3958284854888916\n",
      "Score 3: 0.0471743568778038\n",
      "Image #31\n",
      "Score 0: 6.684188065264607e-07\n",
      "Score 1: 0.08358873426914215\n",
      "Score 2: 8.905716640583705e-06\n",
      "Score 3: 0.05411505326628685\n",
      "Image #32\n",
      "Score 0: 1.2257998605491593e-05\n",
      "Score 1: 0.34069523215293884\n",
      "Score 2: 0.08001111447811127\n",
      "Score 3: 0.05939705669879913\n",
      "Image #33\n",
      "Score 0: 0.004103010054677725\n",
      "Score 1: 0.025474948808550835\n",
      "Score 2: 1.0204992294311523\n",
      "Score 3: 0.08279895037412643\n",
      "Image #34\n",
      "Score 0: 3.2905611991882324\n",
      "Score 1: 4.61615514755249\n",
      "Score 2: 1.9160091876983643\n",
      "Score 3: 0.19116663932800293\n",
      "Image #35\n",
      "Score 0: 0.26960837841033936\n",
      "Score 1: 2.6843819618225098\n",
      "Score 2: 0.005801185965538025\n",
      "Score 3: 13.901994705200195\n",
      "Image #36\n",
      "Score 0: 0.0003612742875702679\n",
      "Score 1: 0.3006245791912079\n",
      "Score 2: 0.00943729467689991\n",
      "Score 3: 0.5696772933006287\n",
      "Image #37\n",
      "Score 0: 3.9360529626719654e-05\n",
      "Score 1: 2.3442697525024414\n",
      "Score 2: 0.5872429609298706\n",
      "Score 3: 0.20057839155197144\n",
      "Image #38\n",
      "Score 0: 0.019316013902425766\n",
      "Score 1: 0.14486244320869446\n",
      "Score 2: 0.7826343774795532\n",
      "Score 3: 1.3711928129196167\n",
      "Image #39\n",
      "Score 0: 0.0016138229984790087\n",
      "Score 1: 7.376194953918457\n",
      "Score 2: 0.008679237216711044\n",
      "Score 3: 8.12881851196289\n",
      "Image #40\n",
      "Score 0: 0.0003691269375849515\n",
      "Score 1: 0.37910470366477966\n",
      "Score 2: 0.002325166016817093\n",
      "Score 3: 0.2091432809829712\n",
      "Image #41\n",
      "Score 0: 0.12852540612220764\n",
      "Score 1: 0.21818497776985168\n",
      "Score 2: 1.9952644109725952\n",
      "Score 3: 0.04364418238401413\n",
      "Image #42\n",
      "Score 0: 0.00033035679371096194\n",
      "Score 1: 0.12079842388629913\n",
      "Score 2: 0.33207079768180847\n",
      "Score 3: 0.08647626638412476\n",
      "Image #43\n",
      "Score 0: 0.02290438860654831\n",
      "Score 1: 0.10460006445646286\n",
      "Score 2: 0.00235080742277205\n",
      "Score 3: 0.08262568712234497\n",
      "Image #44\n",
      "Score 0: 1.3239504098892212\n",
      "Score 1: 4.222925662994385\n",
      "Score 2: 2.682982921600342\n",
      "Score 3: 0.8673604726791382\n",
      "Image #45\n",
      "Score 0: 7.447536063409643e-06\n",
      "Score 1: 1.2722980976104736\n",
      "Score 2: 0.3030933141708374\n",
      "Score 3: 0.2092498242855072\n",
      "Image #46\n",
      "Score 0: 1.6233922243118286\n",
      "Score 1: 1.1941299438476562\n",
      "Score 2: 2.203719139099121\n",
      "Score 3: 0.4192010164260864\n",
      "Image #47\n",
      "Score 0: 0.9015213251113892\n",
      "Score 1: 1.2924952507019043\n",
      "Score 2: 0.07818421721458435\n",
      "Score 3: 0.001370029873214662\n",
      "Image #48\n",
      "Score 0: 0.00012090323434676975\n",
      "Score 1: 0.6585189700126648\n",
      "Score 2: 0.2864065170288086\n",
      "Score 3: 0.43695807456970215\n",
      "Image #49\n",
      "Score 0: 0.12158431857824326\n",
      "Score 1: 0.44639772176742554\n",
      "Score 2: 0.41583508253097534\n",
      "Score 3: 0.4054463505744934\n",
      "Image #50\n",
      "Score 0: 0.0009602003847248852\n",
      "Score 1: 0.19554804265499115\n",
      "Score 2: 0.22112934291362762\n",
      "Score 3: 0.04983735829591751\n",
      "Image #51\n",
      "Score 0: 0.30422234535217285\n",
      "Score 1: 0.8116149306297302\n",
      "Score 2: 0.9294055700302124\n",
      "Score 3: 1.2857577800750732\n",
      "Image #52\n",
      "Score 0: 3.7861480712890625\n",
      "Score 1: 10.68680477142334\n",
      "Score 2: 2.9793758392333984\n",
      "Score 3: 0.1340443640947342\n",
      "Image #53\n",
      "Score 0: 2.9503273963928223\n",
      "Score 1: 10.897284507751465\n",
      "Score 2: 2.1723413467407227\n",
      "Score 3: 0.04940148442983627\n",
      "Image #54\n",
      "Score 0: 0.006822414696216583\n",
      "Score 1: 0.290290892124176\n",
      "Score 2: 1.1562291383743286\n",
      "Score 3: 1.064191460609436\n",
      "Image #55\n",
      "Score 0: 3.657508134841919\n",
      "Score 1: 2.309526205062866\n",
      "Score 2: 4.584022521972656\n",
      "Score 3: 3.354022741317749\n",
      "Image #56\n",
      "Score 0: 3.435679172980599e-05\n",
      "Score 1: 1.432200312614441\n",
      "Score 2: 0.22111289203166962\n",
      "Score 3: 0.6385966539382935\n",
      "Image #57\n",
      "Score 0: 6.841581344604492\n",
      "Score 1: 11.29810905456543\n",
      "Score 2: 4.597450256347656\n",
      "Score 3: 0.5326510071754456\n",
      "Image #58\n",
      "Score 0: 0.15200132131576538\n",
      "Score 1: 0.15173445641994476\n",
      "Score 2: 0.7341409921646118\n",
      "Score 3: 0.49294090270996094\n",
      "Image #59\n",
      "Score 0: 2.553698778152466\n",
      "Score 1: 3.46203351020813\n",
      "Score 2: 5.409194469451904\n",
      "Score 3: 8.956452369689941\n",
      "Image #60\n",
      "Score 0: 7.026285402389476e-06\n",
      "Score 1: 2.508465528488159\n",
      "Score 2: 1.6870982646942139\n",
      "Score 3: 0.17805707454681396\n",
      "Image #61\n",
      "Score 0: 3.3206859370693564e-05\n",
      "Score 1: 0.034592706710100174\n",
      "Score 2: 0.030446050688624382\n",
      "Score 3: 0.009835585951805115\n",
      "Image #62\n",
      "Score 0: 9.26749999052845e-05\n",
      "Score 1: 0.18587765097618103\n",
      "Score 2: 0.053012676537036896\n",
      "Score 3: 0.008034221827983856\n",
      "Image #63\n",
      "Score 0: 9.203546505887061e-05\n",
      "Score 1: 0.3549172580242157\n",
      "Score 2: 0.07542487978935242\n",
      "Score 3: 0.342631459236145\n",
      "Image #64\n",
      "Score 0: 0.00014980298874434084\n",
      "Score 1: 0.2009149193763733\n",
      "Score 2: 0.021675586700439453\n",
      "Score 3: 0.004460086580365896\n",
      "Image #65\n",
      "Score 0: 2.8526470487122424e-05\n",
      "Score 1: 0.03331043943762779\n",
      "Score 2: 0.0006979097379371524\n",
      "Score 3: 0.009034066461026669\n",
      "Image #66\n",
      "Score 0: 0.004401978570967913\n",
      "Score 1: 0.2988440692424774\n",
      "Score 2: 0.32797670364379883\n",
      "Score 3: 0.02201860211789608\n",
      "Image #67\n",
      "Score 0: 5.102151393890381\n",
      "Score 1: 2.0633227825164795\n",
      "Score 2: 5.165226936340332\n",
      "Score 3: 6.220534801483154\n",
      "Image #68\n",
      "Score 0: 0.0002735418966040015\n",
      "Score 1: 5.866581439971924\n",
      "Score 2: 2.6051695346832275\n",
      "Score 3: 0.004033660516142845\n",
      "Image #69\n",
      "Score 0: 0.006940748542547226\n",
      "Score 1: 1.2838983535766602\n",
      "Score 2: 1.073235273361206\n",
      "Score 3: 0.38350909948349\n",
      "Image #70\n",
      "Score 0: 0.8961485624313354\n",
      "Score 1: 4.330633640289307\n",
      "Score 2: 0.4792768955230713\n",
      "Score 3: 0.45533275604248047\n",
      "Image #71\n",
      "Score 0: 0.22166824340820312\n",
      "Score 1: 0.05972255766391754\n",
      "Score 2: 0.4497753083705902\n",
      "Score 3: 0.00466199591755867\n",
      "Image #72\n",
      "Score 0: 0.004220100119709969\n",
      "Score 1: 1.2480010986328125\n",
      "Score 2: 2.551727771759033\n",
      "Score 3: 0.18034350872039795\n",
      "Image #73\n",
      "Score 0: 0.18523690104484558\n",
      "Score 1: 5.932884693145752\n",
      "Score 2: 1.2816842794418335\n",
      "Score 3: 0.6305520534515381\n",
      "Image #74\n",
      "Score 0: 1.4934217688278295e-05\n",
      "Score 1: 1.189009189605713\n",
      "Score 2: 0.6134971976280212\n",
      "Score 3: 0.13368409872055054\n",
      "Image #75\n",
      "Score 0: 0.695112407207489\n",
      "Score 1: 1.3480323553085327\n",
      "Score 2: 1.2273736000061035\n",
      "Score 3: 0.3210817575454712\n",
      "Image #76\n",
      "Score 0: 4.237690518493764e-05\n",
      "Score 1: 2.9274375438690186\n",
      "Score 2: 0.8048245906829834\n",
      "Score 3: 0.8668678998947144\n",
      "Image #77\n",
      "Score 0: 3.558702883310616e-05\n",
      "Score 1: 0.20191755890846252\n",
      "Score 2: 1.6776198148727417\n",
      "Score 3: 0.7472519874572754\n",
      "Image #78\n",
      "Score 0: 0.03874712809920311\n",
      "Score 1: 0.12555313110351562\n",
      "Score 2: 0.038627177476882935\n",
      "Score 3: 0.023075655102729797\n",
      "Image #79\n",
      "Score 0: 0.20409755408763885\n",
      "Score 1: 1.355260968208313\n",
      "Score 2: 0.9115515947341919\n",
      "Score 3: 1.0102394819259644\n",
      "Image #80\n",
      "Score 0: 2.22574520111084\n",
      "Score 1: 8.670391082763672\n",
      "Score 2: 3.993131637573242\n",
      "Score 3: 0.3105798363685608\n",
      "Image #81\n",
      "Score 0: 0.6770594716072083\n",
      "Score 1: 6.303787708282471\n",
      "Score 2: 2.8438854217529297\n",
      "Score 3: 0.024481602013111115\n",
      "Image #82\n",
      "Score 0: 0.14986619353294373\n",
      "Score 1: 0.7158077955245972\n",
      "Score 2: 1.9912784099578857\n",
      "Score 3: 0.03285428509116173\n",
      "IMAGE #0\n",
      "Class name: good\n",
      "Average score: 0.106283225\n",
      "Predictions: 0.0\n",
      "IMAGE #1\n",
      "Class name: good\n",
      "Average score: 0.040252358\n",
      "Predictions: 0.0\n",
      "IMAGE #2\n",
      "Class name: good\n",
      "Average score: 0.58564955\n",
      "Predictions: 1.0\n",
      "IMAGE #3\n",
      "Class name: good\n",
      "Average score: 0.12436353\n",
      "Predictions: 1.0\n",
      "IMAGE #4\n",
      "Class name: good\n",
      "Average score: 0.029171836\n",
      "Predictions: 0.0\n",
      "IMAGE #5\n",
      "Class name: good\n",
      "Average score: 0.23997748\n",
      "Predictions: 1.0\n",
      "IMAGE #6\n",
      "Class name: good\n",
      "Average score: 0.00071796065\n",
      "Predictions: 0.0\n",
      "IMAGE #7\n",
      "Class name: good\n",
      "Average score: 0.036646377\n",
      "Predictions: 0.0\n",
      "IMAGE #8\n",
      "Class name: good\n",
      "Average score: 0.072006434\n",
      "Predictions: 0.0\n",
      "IMAGE #9\n",
      "Class name: good\n",
      "Average score: 0.19841391\n",
      "Predictions: 1.0\n",
      "IMAGE #10\n",
      "Class name: good\n",
      "Average score: 0.027577605\n",
      "Predictions: 0.0\n",
      "IMAGE #11\n",
      "Class name: good\n",
      "Average score: 0.29135185\n",
      "Predictions: 1.0\n",
      "IMAGE #12\n",
      "Class name: good\n",
      "Average score: 0.13058977\n",
      "Predictions: 1.0\n",
      "IMAGE #13\n",
      "Class name: good\n",
      "Average score: 0.10242062\n",
      "Predictions: 0.0\n",
      "IMAGE #14\n",
      "Class name: good\n",
      "Average score: 0.07925407\n",
      "Predictions: 0.0\n",
      "IMAGE #15\n",
      "Class name: good\n",
      "Average score: 0.3464409\n",
      "Predictions: 1.0\n",
      "IMAGE #16\n",
      "Class name: good\n",
      "Average score: 0.14998715\n",
      "Predictions: 1.0\n",
      "IMAGE #17\n",
      "Class name: good\n",
      "Average score: 0.12086296\n",
      "Predictions: 0.0\n",
      "IMAGE #18\n",
      "Class name: good\n",
      "Average score: 0.6104357\n",
      "Predictions: 1.0\n",
      "IMAGE #19\n",
      "Class name: good\n",
      "Average score: 0.28321218\n",
      "Predictions: 1.0\n",
      "IMAGE #20\n",
      "Class name: contamination\n",
      "Average score: 1.9479382\n",
      "Predictions: 1.0\n",
      "IMAGE #21\n",
      "Class name: contamination\n",
      "Average score: 0.98533773\n",
      "Predictions: 1.0\n",
      "IMAGE #22\n",
      "Class name: contamination\n",
      "Average score: 1.0239829\n",
      "Predictions: 1.0\n",
      "IMAGE #23\n",
      "Class name: contamination\n",
      "Average score: 0.9818443\n",
      "Predictions: 1.0\n",
      "IMAGE #24\n",
      "Class name: contamination\n",
      "Average score: 0.04421886\n",
      "Predictions: 0.0\n",
      "IMAGE #25\n",
      "Class name: contamination\n",
      "Average score: 1.6398542\n",
      "Predictions: 1.0\n",
      "IMAGE #26\n",
      "Class name: contamination\n",
      "Average score: 9.742379\n",
      "Predictions: 1.0\n",
      "IMAGE #27\n",
      "Class name: contamination\n",
      "Average score: 1.2351297\n",
      "Predictions: 1.0\n",
      "IMAGE #28\n",
      "Class name: contamination\n",
      "Average score: 1.5166434\n",
      "Predictions: 1.0\n",
      "IMAGE #29\n",
      "Class name: contamination\n",
      "Average score: 3.4845908\n",
      "Predictions: 1.0\n",
      "IMAGE #30\n",
      "Class name: contamination\n",
      "Average score: 0.5360175\n",
      "Predictions: 1.0\n",
      "IMAGE #31\n",
      "Class name: contamination\n",
      "Average score: 0.03442834\n",
      "Predictions: 0.0\n",
      "IMAGE #32\n",
      "Class name: contamination\n",
      "Average score: 0.12002891\n",
      "Predictions: 0.0\n",
      "IMAGE #33\n",
      "Class name: contamination\n",
      "Average score: 0.28321904\n",
      "Predictions: 1.0\n",
      "IMAGE #34\n",
      "Class name: contamination\n",
      "Average score: 2.503473\n",
      "Predictions: 1.0\n",
      "IMAGE #35\n",
      "Class name: contamination\n",
      "Average score: 4.2154465\n",
      "Predictions: 1.0\n",
      "IMAGE #36\n",
      "Class name: contamination\n",
      "Average score: 0.2200251\n",
      "Predictions: 1.0\n",
      "IMAGE #37\n",
      "Class name: contamination\n",
      "Average score: 0.7830326\n",
      "Predictions: 1.0\n",
      "IMAGE #38\n",
      "Class name: contamination\n",
      "Average score: 0.5795014\n",
      "Predictions: 1.0\n",
      "IMAGE #39\n",
      "Class name: contamination\n",
      "Average score: 3.8788266\n",
      "Predictions: 1.0\n",
      "IMAGE #40\n",
      "Class name: contamination\n",
      "Average score: 0.14773557\n",
      "Predictions: 1.0\n",
      "IMAGE #41\n",
      "Class name: broken_large\n",
      "Average score: 0.59640473\n",
      "Predictions: 1.0\n",
      "IMAGE #42\n",
      "Class name: broken_large\n",
      "Average score: 0.13491896\n",
      "Predictions: 1.0\n",
      "IMAGE #43\n",
      "Class name: broken_large\n",
      "Average score: 0.053120237\n",
      "Predictions: 0.0\n",
      "IMAGE #44\n",
      "Class name: broken_large\n",
      "Average score: 2.2743046\n",
      "Predictions: 1.0\n",
      "IMAGE #45\n",
      "Class name: broken_large\n",
      "Average score: 0.44616216\n",
      "Predictions: 1.0\n",
      "IMAGE #46\n",
      "Class name: broken_large\n",
      "Average score: 1.3601105\n",
      "Predictions: 1.0\n",
      "IMAGE #47\n",
      "Class name: broken_large\n",
      "Average score: 0.56839263\n",
      "Predictions: 1.0\n",
      "IMAGE #48\n",
      "Class name: broken_large\n",
      "Average score: 0.34550112\n",
      "Predictions: 1.0\n",
      "IMAGE #49\n",
      "Class name: broken_large\n",
      "Average score: 0.34731585\n",
      "Predictions: 1.0\n",
      "IMAGE #50\n",
      "Class name: broken_large\n",
      "Average score: 0.116868734\n",
      "Predictions: 0.0\n",
      "IMAGE #51\n",
      "Class name: broken_large\n",
      "Average score: 0.83275014\n",
      "Predictions: 1.0\n",
      "IMAGE #52\n",
      "Class name: broken_large\n",
      "Average score: 4.396593\n",
      "Predictions: 1.0\n",
      "IMAGE #53\n",
      "Class name: broken_large\n",
      "Average score: 4.017339\n",
      "Predictions: 1.0\n",
      "IMAGE #54\n",
      "Class name: broken_large\n",
      "Average score: 0.62938344\n",
      "Predictions: 1.0\n",
      "IMAGE #55\n",
      "Class name: broken_large\n",
      "Average score: 3.47627\n",
      "Predictions: 1.0\n",
      "IMAGE #56\n",
      "Class name: broken_large\n",
      "Average score: 0.572986\n",
      "Predictions: 1.0\n",
      "IMAGE #57\n",
      "Class name: broken_large\n",
      "Average score: 5.817448\n",
      "Predictions: 1.0\n",
      "IMAGE #58\n",
      "Class name: broken_large\n",
      "Average score: 0.38270444\n",
      "Predictions: 1.0\n",
      "IMAGE #59\n",
      "Class name: broken_large\n",
      "Average score: 5.0953445\n",
      "Predictions: 1.0\n",
      "IMAGE #60\n",
      "Class name: broken_large\n",
      "Average score: 1.093407\n",
      "Predictions: 1.0\n",
      "IMAGE #61\n",
      "Class name: broken_small\n",
      "Average score: 0.018726887\n",
      "Predictions: 0.0\n",
      "IMAGE #62\n",
      "Class name: broken_small\n",
      "Average score: 0.06175431\n",
      "Predictions: 0.0\n",
      "IMAGE #63\n",
      "Class name: broken_small\n",
      "Average score: 0.1932664\n",
      "Predictions: 1.0\n",
      "IMAGE #64\n",
      "Class name: broken_small\n",
      "Average score: 0.056800097\n",
      "Predictions: 0.0\n",
      "IMAGE #65\n",
      "Class name: broken_small\n",
      "Average score: 0.0107677365\n",
      "Predictions: 0.0\n",
      "IMAGE #66\n",
      "Class name: broken_small\n",
      "Average score: 0.16331033\n",
      "Predictions: 1.0\n",
      "IMAGE #67\n",
      "Class name: broken_small\n",
      "Average score: 4.637809\n",
      "Predictions: 1.0\n",
      "IMAGE #68\n",
      "Class name: broken_small\n",
      "Average score: 2.1190147\n",
      "Predictions: 1.0\n",
      "IMAGE #69\n",
      "Class name: broken_small\n",
      "Average score: 0.68689585\n",
      "Predictions: 1.0\n",
      "IMAGE #70\n",
      "Class name: broken_small\n",
      "Average score: 1.540348\n",
      "Predictions: 1.0\n",
      "IMAGE #71\n",
      "Class name: broken_small\n",
      "Average score: 0.18395703\n",
      "Predictions: 1.0\n",
      "IMAGE #72\n",
      "Class name: broken_small\n",
      "Average score: 0.9960731\n",
      "Predictions: 1.0\n",
      "IMAGE #73\n",
      "Class name: broken_small\n",
      "Average score: 2.0075896\n",
      "Predictions: 1.0\n",
      "IMAGE #74\n",
      "Class name: broken_small\n",
      "Average score: 0.48405135\n",
      "Predictions: 1.0\n",
      "IMAGE #75\n",
      "Class name: broken_small\n",
      "Average score: 0.8979\n",
      "Predictions: 1.0\n",
      "IMAGE #76\n",
      "Class name: broken_small\n",
      "Average score: 1.1497931\n",
      "Predictions: 1.0\n",
      "IMAGE #77\n",
      "Class name: broken_small\n",
      "Average score: 0.6567062\n",
      "Predictions: 1.0\n",
      "IMAGE #78\n",
      "Class name: broken_small\n",
      "Average score: 0.056500774\n",
      "Predictions: 0.0\n",
      "IMAGE #79\n",
      "Class name: broken_small\n",
      "Average score: 0.8702874\n",
      "Predictions: 1.0\n",
      "IMAGE #80\n",
      "Class name: broken_small\n",
      "Average score: 3.799962\n",
      "Predictions: 1.0\n",
      "IMAGE #81\n",
      "Class name: broken_small\n",
      "Average score: 2.4623036\n",
      "Predictions: 1.0\n",
      "IMAGE #82\n",
      "Class name: broken_small\n",
      "Average score: 0.7224517\n",
      "Predictions: 1.0\n",
      "AUC: 0.8261904761904761\n",
      "F1: 0.8412698412698413\n",
      "Average Precision: 0.9471935495011063\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (256,256)\n",
    "parser = argparse.ArgumentParser(description='Contrastive predictive coding params')\n",
    "\n",
    "# mode = 'train_encoder_context_prediction'\n",
    "# mode = 'anomaly_evaluation'\n",
    "parser.add_argument('-mode', default='anomaly_evaluation' , type=str)\n",
    "# chest-xray-pneumonia/chest_xray/train' \n",
    "# mvtec-ad/bottle/train'          num_class 1         \n",
    "parser.add_argument('-train_image_folder', default='../input/mvtec-ad/grid/train', type=str)\n",
    "parser.add_argument('-num_classes_train', default=1, type=int)\n",
    "# chest-xray-pneumonia/chest_xray/train'  num_class 2\n",
    "# mvtec-ad/bottle/test'                   num_class 4\n",
    "parser.add_argument('-test_image_folder', default='../input/mvtec-ad/bottle/test', type=str)\n",
    "parser.add_argument('-num_classes_test', default=4, type=int)\n",
    "parser.add_argument('-batch_size', default=16, type=int)\n",
    "parser.add_argument('-sub_batch_size', default=2, type=int)\n",
    "parser.add_argument('-num_random_patches', default=15, type=int)\n",
    "parser.add_argument('-num_epochs', default=30, type=int)\n",
    "parser.add_argument('-patience', default=10, type=int)\n",
    "\n",
    "parser.add_argument('-layers', default=100, type=int,\n",
    "                    help='total number of layers (default: 100)')\n",
    "parser.add_argument('-growth', default=12, type=int,\n",
    "                    help='number of new channels per layer (default: 12)')\n",
    "parser.add_argument('-reduce', default=0.5, type=float,\n",
    "                    help='compression rate in transition stage (default: 0.5)')\n",
    "parser.add_argument('-no-bottleneck', dest='bottleneck', action='store_false',\n",
    "                    help='To not use bottleneck block')\n",
    "parser.add_argument('-droprate', default=0, type=float,\n",
    "                    help='dropout probability (default: 0.0)')\n",
    "# cpu or cuda\n",
    "parser.add_argument('-device', default='cuda', type=str)\n",
    "\n",
    "args, args_other = parser.parse_known_args()\n",
    "\n",
    "print(f\"Running CPC with args {args}\")\n",
    "\n",
    "Z_DIMENSIONS = 1024\n",
    "DIRECTIONS = ['DOWN', 'UP', 'RIGHT', 'LEFT']\n",
    "\n",
    "stored_models_root_path = \"trained_models\"\n",
    "if not os.path.isdir(stored_models_root_path):\n",
    "    os.mkdir(stored_models_root_path)\n",
    "stored_eval_root_path = \"evaluation\"\n",
    "if not os.path.isdir(stored_eval_root_path):\n",
    "    os.mkdir(stored_eval_root_path)\n",
    "\n",
    "if args.mode == 'train_encoder_context_prediction':\n",
    "    \n",
    "    for i, direc in enumerate(DIRECTIONS):\n",
    "        denseNet_model = None\n",
    "        context_predictor_model = None\n",
    "\n",
    "        # create model\n",
    "        denseNet_model = DenseNet3(args.layers, args.growth, reduction=args.reduce,\n",
    "                             bottleneck=args.bottleneck, dropRate=args.droprate).to(args.device)\n",
    "        \n",
    "        #res_encoder_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device)\n",
    "        context_predictor_model = ContextPredictionModelWithDir(in_channels=Z_DIMENSIONS, direction=direc).to(args.device)\n",
    "        \n",
    "        # Models training\n",
    "        model_store_folder = get_next_model_folder(direc, stored_models_root_path)\n",
    "        os.mkdir(model_store_folder)\n",
    "        run_context_predictor(args, denseNet_model, context_predictor_model, model_store_folder)\n",
    "\n",
    "if args.mode == 'anomaly_evaluation':\n",
    "    # Evaluation\n",
    "    denseNet_model_list = []\n",
    "    context_model_list = []\n",
    "    \n",
    "    for i, direc in enumerate(DIRECTIONS):\n",
    "        denseNet_model = None\n",
    "        context_predictor_model = None\n",
    "        res_encoder_weights_path = ''\n",
    "        context_weights_path = ''\n",
    "        #res_encoder_model = ResEncoderModel().to(args.device)\n",
    "        # ResNet18 v2 up to the third residual block\n",
    "        denseNet_model = DenseNet3(args.layers, args.growth, reduction=args.reduce,\n",
    "                             bottleneck=args.bottleneck, dropRate=args.droprate).to(args.device)\n",
    "        context_predictor_model = ContextPredictionModelWithDir(in_channels=Z_DIMENSIONS, direction=direc).to(args.device)\n",
    "        \n",
    "        if direc == 'DOWN':\n",
    "            res_encoder_weights_path = '/kaggle/input/outputtrain/DOWN_best_res_encoder_weights.pt'\n",
    "            context_weights_path = '/kaggle/input/outputtrain/DOWN_best_context_weights.pt'\n",
    "        elif direc == 'UP':\n",
    "            res_encoder_weights_path = '/kaggle/input/outputtrain/UP_best_res_encoder_weights.pt'\n",
    "            context_weights_path = '/kaggle/input/outputtrain/UP_best_context_weights.pt'\n",
    "        elif direc == 'RIGHT':\n",
    "            res_encoder_weights_path = '/kaggle/input/outputtrain/RIGHT_best_res_encoder_weights.pt'\n",
    "            context_weights_path = '/kaggle/input/outputtrain/RIGHT_best_context_weights.pt'\n",
    "        else:\n",
    "            res_encoder_weights_path = '/kaggle/input/outputtrain/LEFT_best_res_encoder_weights.pt'\n",
    "            context_weights_path = '/kaggle/input/outputtrain/LEFT_best_context_weights.pt'\n",
    "        \n",
    "        print(f\"Loading res encoder {direc} weights from {res_encoder_weights_path}\")\n",
    "        print(f\"Loading context {direc} weights from {context_weights_path}\")\n",
    "        \n",
    "        # Load weights in the models\n",
    "        denseNet_model.load_state_dict(torch.load(res_encoder_weights_path))\n",
    "        context_predictor_model.load_state_dict(torch.load(context_weights_path))\n",
    "        \n",
    "        # Encoder models and context models lists\n",
    "        denseNet_model_list.append(denseNet_model)\n",
    "        context_model_list.append(context_predictor_model)\n",
    "        \n",
    "    run_anomaly_evaluation(args, denseNet_model_list, context_model_list, stored_eval_root_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 115.715885,
   "end_time": "2023-01-09T22:08:42.581987",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-09T22:06:46.866102",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
