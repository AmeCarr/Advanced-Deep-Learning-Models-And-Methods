{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import Module\nimport datetime\nimport os\nimport argparse\nimport random\nimport csv\nimport math\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nimport numpy as np\nimport time\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support, average_precision_score\nimport torch.nn.functional as F","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-12T18:14:59.643899Z","iopub.execute_input":"2023-01-12T18:14:59.644908Z","iopub.status.idle":"2023-01-12T18:14:59.651780Z","shell.execute_reply.started":"2023-01-12T18:14:59.644869Z","shell.execute_reply":"2023-01-12T18:14:59.650385Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# **Helper functions**","metadata":{}},{"cell_type":"code","source":"def dot(a,b):\n    dot = torch.sum(a * b, dim=1)\n    return dot\n\ndef get_next_model_folder(prefix, path = ''):\n\n    model_folder = lambda prefix, run_idx: f\"{prefix}_model_run_{run_idx}\"\n\n    run_idx = 1\n    while os.path.isdir(os.path.join(path, model_folder(prefix, run_idx))):\n        run_idx += 1\n\n    model_path = os.path.join(path, model_folder(prefix, run_idx))\n    print(f\"STARTING {prefix} RUN {run_idx}! Storing the models at {model_path}\")\n\n    return model_path\n\ndef get_random_patches(random_patch_loader, num_random_patches):\n\n        is_data_loader_finished = False\n\n        try:\n            img_batch = next(iter(random_patch_loader))['image']\n        except StopIteration:\n            is_data_loader_finished = True\n\n        if len(img_batch) < num_random_patches:\n            is_data_loader_finished = True\n\n        patches = []\n\n        for i in range(num_random_patches):\n            x = random.randint(0,6)\n            y = random.randint(0,6)\n\n            patches.append(img_batch[i:i+1,:,x*32:x*32+64,y*32:y*32+64])\n\n        patches_tensor = torch.cat(patches, dim=0)\n\n        return dict(\n            patches_tensor = patches_tensor,\n            is_data_loader_finished = is_data_loader_finished)\n\n# Tell how many parameters are on the model\ndef inspect_model(model):\n    param_count = 0\n    for param_tensor_str in model.state_dict():\n        tensor_size = model.state_dict()[param_tensor_str].size()\n        print(f\"{param_tensor_str} size {tensor_size} = {model.state_dict()[param_tensor_str].numel()} params\")\n        param_count += model.state_dict()[param_tensor_str].numel()\n\n    print(f\"Number of parameters: {param_count}\")\n    \ndef get_patch_tensor_from_image_batch(img_batch):\n\n    # Input of the function is a tensor [B, C, H, W]\n    # Output of the functions is a tensor [B * 49, C, 64, 64]\n\n    patch_batch = None\n    all_patches_list = []\n\n    for y_patch in range(7):\n        for x_patch in range(7):\n\n            y1 = y_patch * 32\n            y2 = y1 + 64\n\n            x1 = x_patch * 32\n            x2 = x1 + 64\n\n            img_patches = img_batch[:,:,y1:y2,x1:x2] # Batch(img_idx in batch), channels xrange, yrange\n            img_patches = img_patches.unsqueeze(dim=1)\n            all_patches_list.append(img_patches)\n\n            # print(patch_batch.shape)\n    all_patches_tensor = torch.cat(all_patches_list, dim=1)\n\n    patches_per_image = []\n    for b in range(all_patches_tensor.shape[0]):\n        patches_per_image.append(all_patches_tensor[b])\n\n    patch_batch = torch.cat(patches_per_image, dim = 0)\n    return patch_batch\n\ndef compute_pre_recall_f1(target, pred):\n    precision, recall, f1, _ = precision_recall_fscore_support(target, pred, average='binary')\n    return f1","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.679418Z","iopub.execute_input":"2023-01-12T18:14:59.679713Z","iopub.status.idle":"2023-01-12T18:14:59.696879Z","shell.execute_reply.started":"2023-01-12T18:14:59.679686Z","shell.execute_reply":"2023-01-12T18:14:59.695897Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset utils**","metadata":{}},{"cell_type":"code","source":"class ImageNetDataset(Dataset):\n    def __init__(self, data_path, is_train, random_seed = 42, target_transform = None, num_classes = None):\n        super(ImageNetDataset, self).__init__()\n        self.data_path = data_path\n\n        self.is_classes_limited = False\n\n        if num_classes != None:\n            self.is_classes_limited = True\n            self.num_classes = num_classes\n\n        self.classes = []\n        class_idx = 0\n        for class_name in os.listdir(data_path):\n            if not os.path.isdir(os.path.join(data_path,class_name)):\n                continue\n            self.classes.append(\n               dict(\n                   class_idx = class_idx,\n                   class_name = class_name,\n               ))\n            class_idx += 1\n\n            if self.is_classes_limited:\n                if class_idx == self.num_classes:\n                    break\n\n        if not self.is_classes_limited:\n            self.num_classes = len(self.classes)\n\n        self.image_list = []\n        for cls in self.classes:\n            class_path = os.path.join(data_path, cls['class_name'])\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                self.image_list.append(dict(\n                    cls = cls,\n                    image_path = image_path,\n                    image_name = image_name,\n                ))\n\n        self.img_idxes = np.arange(0,len(self.image_list))\n\n        np.random.seed(random_seed)\n\n        if is_train:\n            np.random.shuffle(self.img_idxes)\n        last_train_sample = int(len(self.img_idxes))\n        self.img_idxes = self.img_idxes[:last_train_sample]\n\n    def __len__(self):\n        return len(self.img_idxes)\n\n    def __getitem__(self, index):\n\n        img_idx = self.img_idxes[index]\n        img_info = self.image_list[img_idx]\n\n        img = Image.open(img_info['image_path'])\n        #print('IMG MODE: ' + str(img.mode))\n\n        if img.mode == 'L':\n            tr = transforms.Grayscale(num_output_channels=3)\n            img = tr(img)\n\n        tr = transforms.ToTensor()\n        img1 = tr(img)\n\n        width, height = img.size\n        if min(width, height)>IMG_SIZE[0] * 1.5:\n            tr = transforms.Resize(int(IMG_SIZE[0] * 1.5))\n            img = tr(img)\n\n        width, height = img.size\n        if min(width, height)<IMG_SIZE[0]:\n            tr = transforms.Resize(IMG_SIZE)\n            img = tr(img)\n\n        tr = transforms.RandomCrop(IMG_SIZE)\n        img = tr(img)\n\n        tr = transforms.ToTensor()\n        img = tr(img)\n\n        if (img.shape[0] != 3):\n            img = img[0:3]\n\n        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n\n    def get_number_of_classes(self):\n        return self.num_classes\n\n    def get_number_of_samples(self):\n        return self.__len__()\n\n    def get_class_names(self):\n        return [cls['class_name'] for cls in self.classes]\n\n    def get_class_name(self, class_idx):\n        return self.classes[class_idx]['class_name']\n\n\ndef get_imagenet_datasets(train_path, test_path, train_split = 0.9, num_classes_train = None, num_classes_test = None, random_seed = None):\n\n    if random_seed == None:\n        random_seed = int(time.time())\n    dataset_train = ImageNetDataset(train_path, is_train = True, random_seed=random_seed, num_classes = num_classes_train)\n    trainset_size = int(len(dataset_train)*train_split)\n    validset_size = len(dataset_train) - trainset_size\n    dataset_train, dataset_valid = random_split(dataset_train, [trainset_size, validset_size])\n    dataset_test = ImageNetDataset(test_path, is_train = False, random_seed=random_seed, num_classes = num_classes_test)\n\n    return dataset_train, dataset_valid, dataset_test\n    \ndef get_random_patch_loader(dataset_train):\n    return DataLoader(dataset_train, args.num_random_patches, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.700525Z","iopub.execute_input":"2023-01-12T18:14:59.700796Z","iopub.status.idle":"2023-01-12T18:14:59.721917Z","shell.execute_reply.started":"2023-01-12T18:14:59.700763Z","shell.execute_reply":"2023-01-12T18:14:59.720972Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# **ResNet Blocks**","metadata":{}},{"cell_type":"markdown","source":"* **ResNet18 block**","metadata":{}},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample):\n        super().__init__()\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        if downsample:\n            self.conv1 = nn.Conv2d(\n                in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n                #nn.MaxPool2d(2,2);\n            )\n        else:\n            self.conv1 = nn.Conv2d(\n                in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            self.shortcut = nn.Sequential()\n        \n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels,\n                               kernel_size=3, stride=1, padding=1)\n        \n\n    def forward(self, input):\n        shortcut = self.shortcut(input)\n        input = self.conv1(nn.ReLU()(self.bn1(input)))\n        input = self.conv2(nn.ReLU()(self.bn2(input)))\n        input = input + shortcut\n        return input","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.726208Z","iopub.execute_input":"2023-01-12T18:14:59.726891Z","iopub.status.idle":"2023-01-12T18:14:59.738938Z","shell.execute_reply.started":"2023-01-12T18:14:59.726858Z","shell.execute_reply":"2023-01-12T18:14:59.738008Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"* **ResNet50 block**","metadata":{}},{"cell_type":"code","source":"class ResBottleneckBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample):\n        super().__init__()\n        self.downsample = downsample\n        self.conv1 = nn.Conv2d(in_channels, out_channels//4,\n                               kernel_size=1, stride=1)\n        self.conv2 = nn.Conv2d(\n            out_channels//4, out_channels//4, kernel_size=3, stride=2 if downsample else 1, padding=1)\n        self.conv3 = nn.Conv2d(out_channels//4, out_channels, kernel_size=1, stride=1)\n\n        if self.downsample or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n                          stride=2 if self.downsample else 1),\n                nn.BatchNorm2d(out_channels)\n            )\n        else:\n            self.shortcut = nn.Sequential()\n\n        self.bn1 = nn.BatchNorm2d(in_channels) \n        self.bn2 = nn.BatchNorm2d(out_channels//4)\n        self.bn3 = nn.BatchNorm2d(out_channels//4)\n\n    def forward(self, input):\n        shortcut = self.shortcut(input)\n        input = self.conv1(nn.ReLU()(self.bn1(input)))\n        input = self.conv2(nn.ReLU()(self.bn2(input)))\n        input = self.conv3(nn.ReLU()(self.bn3(input)))\n        input = input + shortcut\n        return nn.ReLU()(input)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.747004Z","iopub.execute_input":"2023-01-12T18:14:59.747394Z","iopub.status.idle":"2023-01-12T18:14:59.758407Z","shell.execute_reply.started":"2023-01-12T18:14:59.747366Z","shell.execute_reply":"2023-01-12T18:14:59.757592Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# **Encoder models**","metadata":{}},{"cell_type":"markdown","source":"* **ResNet18**","metadata":{}},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, in_channels, resblock, repeat, useBottleneck=False, outputs=1024):\n        super().__init__()\n        self.layer0 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        \n        if useBottleneck:\n            filters = [64, 256, 512, 1024] #, 2048]\n        else:\n            filters = [64, 64, 128, 256] #, 512]\n\n        self.layer1 = nn.Sequential()\n        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n        for i in range(1, repeat[0]):\n                self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n\n        self.layer2 = nn.Sequential()\n        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n        for i in range(1, repeat[1]):\n                self.layer2.add_module('conv3_%d' % (\n                    i+1,), resblock(filters[2], filters[2], downsample=False))\n\n        self.layer3 = nn.Sequential()\n        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n        for i in range(1, repeat[2]):\n            self.layer3.add_module('conv2_%d' % (\n                i+1,), resblock(filters[3], filters[3], downsample=False))\n\n        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n        self.fc = torch.nn.Linear(filters[3], outputs)\n        \n    def forward(self, input):\n        input = self.layer0(input)\n        input = self.layer1(input)\n        input = self.layer2(input)\n        input = self.layer3(input)\n        input = self.gap(input)\n        input = torch.flatten(input, start_dim=1)\n        input = self.fc(input)\n\n        return input","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.780862Z","iopub.execute_input":"2023-01-12T18:14:59.781515Z","iopub.status.idle":"2023-01-12T18:14:59.795315Z","shell.execute_reply.started":"2023-01-12T18:14:59.781490Z","shell.execute_reply":"2023-01-12T18:14:59.794501Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"*  **DenseNet**","metadata":{}},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, dropRate=0.0):\n        super(BasicBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n    def forward(self, x):\n        out = self.conv1(self.relu(self.bn1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, training=self.training)\n        return torch.cat([x, out], 1)\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, dropRate=0.0):\n        super(BottleneckBlock, self).__init__()\n        inter_planes = out_planes * 4\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, inter_planes, kernel_size=1, stride=1,\n                               padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(inter_planes)\n        self.conv2 = nn.Conv2d(inter_planes, out_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        self.droprate = dropRate\n    def forward(self, x):\n        out = self.conv1(self.relu(self.bn1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n        out = self.conv2(self.relu(self.bn2(out)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n        return torch.cat([x, out], 1)\n\nclass TransitionBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, dropRate=0.0):\n        super(TransitionBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1,\n                               padding=0, bias=False)\n        self.droprate = dropRate\n    def forward(self, x):\n        out = self.conv1(self.relu(self.bn1(x)))\n        if self.droprate > 0:\n            out = F.dropout(out, p=self.droprate, inplace=False, training=self.training)\n        return F.avg_pool2d(out, 2)\n\nclass DenseBlock(nn.Module):\n    def __init__(self, nb_layers, in_planes, growth_rate, block, dropRate=0.0):\n        super(DenseBlock, self).__init__()\n        self.layer = self._make_layer(block, in_planes, growth_rate, nb_layers, dropRate)\n    def _make_layer(self, block, in_planes, growth_rate, nb_layers, dropRate):\n        layers = []\n        for i in range(nb_layers):\n            layers.append(block(in_planes+i*growth_rate, growth_rate, dropRate))\n        return nn.Sequential(*layers)\n    def forward(self, x):\n        return self.layer(x)\n\nclass DenseNet3(nn.Module):\n    def __init__(self, depth, growth_rate=12,\n                 reduction=0.5, bottleneck=True, dropRate=0.0):\n        super(DenseNet3, self).__init__()\n        in_planes = 2 * growth_rate\n        n = (depth - 4) / 3\n        if bottleneck == True:\n            n = n/2\n            block = BottleneckBlock\n        else:\n            block = BasicBlock\n        n = int(n)\n        # 1st conv before any dense block\n        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3, stride=1,\n                               padding=1, bias=False)\n        # 1st block\n        self.block1 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n        in_planes = int(in_planes+n*growth_rate)\n        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n        in_planes = int(math.floor(in_planes*reduction))\n        # 2nd block\n        self.block2 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n        in_planes = int(in_planes+n*growth_rate)\n        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)), dropRate=dropRate)\n        in_planes = int(math.floor(in_planes*reduction))\n        # 3rd block\n        self.block3 = DenseBlock(n, in_planes, growth_rate, block, dropRate)\n        in_planes = int(in_planes+n*growth_rate)\n        self.conv2 = nn.Conv2d(in_channels=342, out_channels=256, kernel_size=3)\n        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n        self.fc = torch.nn.Linear(256, 1024)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n                \n    def forward(self, x):\n        \n        out = self.conv1(x)\n        out = self.trans1(self.block1(out))\n        out = self.trans2(self.block2(out))\n        out = self.block3(out)        \n        out = self.conv2(out)        \n        out = F.avg_pool2d(out, 8)\n        out = self.gap(out)\n        out = torch.flatten(out, start_dim=1)\n        out = self.fc(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.799934Z","iopub.execute_input":"2023-01-12T18:14:59.800204Z","iopub.status.idle":"2023-01-12T18:14:59.828496Z","shell.execute_reply.started":"2023-01-12T18:14:59.800181Z","shell.execute_reply":"2023-01-12T18:14:59.827552Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# **Context Prediction Model**","metadata":{}},{"cell_type":"markdown","source":"* **Our version of CPC with PixelCNN (autoregressive model)**","metadata":{}},{"cell_type":"code","source":"class ContextPredictionModelWithAutoregressive(Module):\n\n    def __init__(self, in_channels, direction):\n        super(ContextPredictionModelWithAutoregressive, self).__init__()\n\n        self.in_channels = in_channels\n        self.direction = direction\n\n        # Input will be 1024x7x7\n\n        self.context_layers = 3\n        self.context_conv = nn.Sequential()\n\n        for layer_idx in range(self.context_layers):\n            self.context_conv.add_module(f'batch_norm_{layer_idx}',nn.BatchNorm2d(self.in_channels)),\n            self.context_conv.add_module(f'relu_{layer_idx}',nn.ReLU())\n            self.context_conv.add_module(\n                f'conv2d_{layer_idx}',\n                nn.Conv2d(\n                    in_channels = self.in_channels,\n                    out_channels = self.in_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0\n                )\n            )\n\n        self.context_conv.add_module(\n            'adaptive_avg_pool',\n            nn.AdaptiveAvgPool2d(output_size=1)\n        )\n        \n\n        self.prediction_weights = nn.ModuleList([nn.Linear(\n                        in_features = self.in_channels,\n                        out_features = self.in_channels,\n                    ) for i in range(4)])\n        \n\n    # x: encoded patches (2, 1024, 7, 7)\n    def forward(self, x): \n\n        z_patches_loc_list = []\n        context_vectors_list = []\n\n        for y1 in range(3): #rows\n            z_patches_list = []\n            for x1 in range(7): #columns\n\n                z_patches = x[:,:,0:y1+1,0:7] #2, 1024, o 1 o 2 o 3, 7\n                z_patches_loc = (y1,x1) # Store pixel coordinates\n\n                z_patches_list.append(z_patches) # itera fino 7\n                z_patches_loc_list += [z_patches_loc] * len(z_patches)\n\n            z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, o 1 o 2 o 3, 3\n\n            # Apply context model to encoded patches \n            context_vectors_temp = self.context_conv.forward(z_patches_tensor) #14, 1024, 1, 1\n            context_vectors_list.append(context_vectors_temp) # 3\n            \n\n        context_vectors = torch.cat(context_vectors_list, dim = 0) #42, 1024, 1, 1\n        context_vectors = context_vectors.squeeze(dim=3)\n        context_vectors = context_vectors.squeeze(dim=2)\n\n        context_loc_list = torch.tensor(z_patches_loc_list)\n\n        all_predictions = []\n        all_loc = []\n\n        for steps_y in range(4):\n            predictions = self.prediction_weights[steps_y].forward(context_vectors) #42, 1024\n            all_predictions.append(predictions)\n            steps_add = torch.tensor([steps_y + 1,0])\n            all_loc.append(context_loc_list + steps_add)\n\n        ret = torch.cat(all_predictions, dim = 0), torch.cat(all_loc, dim = 0)\n\n        return ret","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.830279Z","iopub.execute_input":"2023-01-12T18:14:59.830663Z","iopub.status.idle":"2023-01-12T18:14:59.845925Z","shell.execute_reply.started":"2023-01-12T18:14:59.830629Z","shell.execute_reply":"2023-01-12T18:14:59.844997Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"* **Model used to predict values for all four directions**","metadata":{}},{"cell_type":"code","source":"class ContextPredictionModelWithDir(Module):\n\n    def __init__(self, in_channels, direction):\n        super(ContextPredictionModelWithDir, self).__init__()\n        # Input will be 1024x7x7\n        self.in_channels = in_channels\n        self.direction = direction\n        \n        self.prediction_weights = nn.ModuleList([nn.Linear(\n                        in_features = self.in_channels,\n                        out_features = self.in_channels,\n                    ) for i in range(4)])\n        \n\n    # x: encoded patches (2, 1024, 7, 7)\n    def forward(self, x): \n\n        z_patches_loc_list = []\n        context_vectors_list = []\n\n        if self.direction == 'DOWN' or self.direction == 'UP':\n            for y1 in range(3): #rows\n                z_patches_list = []\n                for x1 in range(7): #columns\n                    if self.direction == 'DOWN':\n                        z_patches = x[:, :, y1:y1+1, x1:x1+1] #2, 1024, 1, 1\n                        z_patches_loc = (y1,x1) # Store pixel coordinates\n                    else:\n                        z_patches = x[:, :, y1+4:y1+5, x1:x1+1] #2, 1024, 1, 1\n                        z_patches_loc = (y1+4,x1) # Store pixel coordinates\n                    z_patches_list.append(z_patches) \n                    z_patches_loc_list += [z_patches_loc] * len(z_patches)\n\n                z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, 1, 1\n                z_patches_list.append(z_patches_tensor) # 3\n        else:\n            for y1 in range(7): #rows\n                z_patches_list = []\n                for x1 in range(3): #columns\n                    if self.direction == 'RIGHT':\n                        z_patches = x[:, :, y1:y1+1, x1:x1+1] #2, 1024, 1, 1\n                        z_patches_loc = (y1,x1) # Store pixel coordinates\n                    else:\n                        z_patches = x[:, :, y1:y1+1, x1+4:x1+5] #2, 1024, 1, 1\n                        z_patches_loc = (y1,x1+4) # Store pixel coordinates\n                    z_patches_list.append(z_patches) \n                    z_patches_loc_list += [z_patches_loc] * len(z_patches)\n\n                z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, 1, 1\n                z_patches_list.append(z_patches_tensor) # 3\n            \n        z_patches = torch.cat(z_patches_list, dim = 0) #42, 1024, 1, 1\n        z_patches = z_patches.squeeze(dim=3)\n        z_patches = z_patches.squeeze(dim=2)\n\n        context_loc_list = torch.tensor(z_patches_loc_list)\n\n        all_predictions = []\n        all_loc = []\n\n        for steps_y in range(4):\n            predictions = self.prediction_weights[steps_y].forward(z_patches) #42, 1024\n            all_predictions.append(predictions)\n            if self.direction == 'DOWN':\n                steps_add = torch.tensor([steps_y + 1, 0])\n            elif self.direction == 'UP':\n                steps_add = torch.tensor([0 - 1 - steps_y, 0])\n            elif self.direction == 'RIGHT':\n                steps_add = torch.tensor([0, steps_y + 1])\n            else:\n                steps_add = torch.tensor([0, 0 - 1 - steps_y])\n            all_loc.append(context_loc_list + steps_add)\n\n        ret = torch.cat(all_predictions, dim = 0), torch.cat(all_loc, dim = 0)\n\n        return ret","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.847670Z","iopub.execute_input":"2023-01-12T18:14:59.848251Z","iopub.status.idle":"2023-01-12T18:14:59.866821Z","shell.execute_reply.started":"2023-01-12T18:14:59.848218Z","shell.execute_reply":"2023-01-12T18:14:59.866018Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# **Context predictor training**","metadata":{}},{"cell_type":"code","source":"def run_validation(args, res_encoder_model, context_predictor_model, random_patch_loader, data_loader_train, data_loader_valid):\n    res_encoder_model.eval()\n    context_predictor_model.eval()\n    \n    loss_total = 0\n    for i, data in enumerate(data_loader_valid):\n        img = data['image'].to(args.device)\n        patches = get_patch_tensor_from_image_batch(img)\n        \n        with torch.no_grad():\n            patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n            if patches_return['is_data_loader_finished']:\n                random_patch_loader = get_random_patch_loader(dataset_train)\n            else:\n                random_patches = patches_return['patches_tensor'].to(args.device)\n            \n            enc_patches = res_encoder_model(patches)\n            enc_patches = enc_patches.view(1,7,7,-1)\n            enc_patches = enc_patches.permute(0,3,1,2)\n            \n            predictions, locations = context_predictor_model(enc_patches)\n            \n            enc_random_patches = res_encoder_model(random_patches)\n            \n            loss = calculate_score_dir(enc_patches, predictions, locations, enc_random_patches)\n            loss_total += loss\n            \n    return loss_total / len(data_loader_valid)         \n\ndef run_context_predictor(args, res_encoder_model, context_predictor_model, models_store_path):\n\n    print(\"RUNNING CONTEXT PREDICTOR \" +str(context_predictor_model.direction)+ \" TRAINING\")\n    \n    #used to create the file where model weights are saved\n    prefix = str(context_predictor_model.direction)\n    best_encoder = lambda prefix: f\"{prefix}_best_res_encoder_weights.pt\"\n    best_context = lambda prefix: f\"{prefix}_best_context_weights.pt\"\n    \n    #upload of datasets\n    dataset_train, dataset_valid, dataset_test = get_imagenet_datasets(args.train_image_folder, args.test_image_folder, num_classes_train = args.num_classes_train, num_classes_test = args.num_classes_test)\n\n    #creation of dataloaders\n    random_patch_loader = get_random_patch_loader(dataset_train)\n    data_loader_train = DataLoader(dataset_train, args.sub_batch_size, shuffle = True)\n    data_loader_valid = DataLoader(dataset_valid, 1, shuffle = True)\n\n    params = list(res_encoder_model.parameters()) + list(context_predictor_model.parameters())\n    optimizer = torch.optim.Adam(params = params, lr=0.00001)\n\n    trigger = 0\n    patience = args.patience\n    sub_batches_processed = 0\n    batch_loss = 0\n    sum_batch_loss = 0 \n    best_batch_loss = 1e10\n    best_valid_loss = 1e10\n    \n    for epoch in range(1, args.num_epochs + 1):\n        \n        print(\"RUNNING EPOCH #\" + str(epoch))\n        res_encoder_model.train()\n        context_predictor_model.train()\n        \n        for batch in data_loader_train:\n\n            img_batch = batch['image'].to(args.device)\n            patch_batch = get_patch_tensor_from_image_batch(img_batch)\n            batch_size = len(img_batch)\n\n            # Apply encoder to all the 49 patches of the image (64x64)\n            patches_encoded = res_encoder_model.forward(patch_batch) #98, 1024\n            patches_encoded = patches_encoded.view(batch_size, 7,7,-1) #reshape 2, 7, 7, 1024\n            patches_encoded = patches_encoded.permute(0,3,1,2) #2, 1024, 7, 7\n\n            for i in range(2):\n                patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n                if patches_return['is_data_loader_finished']:\n                    random_patch_loader = get_random_patch_loader(dataset_train)\n                else:\n                    random_patches = patches_return['patches_tensor'].to(args.device)\n\n            # Apply encoder to few rendom patches\n            enc_random_patches = res_encoder_model.forward(random_patches)\n\n            # Apply context_predictor to encoded patches\n            predictions, locations = context_predictor_model.forward(patches_encoded) #112, 1024\n            losses = []\n\n            for b in range(len(predictions)//batch_size): #batch_size = 2\n\n                b_idx_start = b*batch_size\n                b_idx_end = (b+1)*batch_size\n\n                p_y = locations[b_idx_start][0]\n                p_x = locations[b_idx_start][1]\n\n                # Encoded patches on the same position of the predictions (Z_i+k,j)\n                target = patches_encoded[:,:,p_y,p_x]\n                # Predicted patches done by context predictor (Zcap_i+k,j = W_k * c_i,j)\n                pred = predictions[b_idx_start:b_idx_end] #2,1024\n\n                # Moltiplication between predictions and encoded patches (Zcap_i+k,j * Z_i+k,j)\n                good_term_dot = dot(pred, target) \n                dot_terms = [torch.unsqueeze(good_term_dot,dim=0)]\n\n                for random_patch_idx in range(args.num_random_patches):\n                    # Moltiplication between predictions and ancoded random patches (Zcap_i+k,j * Z_l)\n                    bad_term_dot = dot(pred, enc_random_patches[random_patch_idx:random_patch_idx+1])\n                    dot_terms.append(torch.unsqueeze(bad_term_dot, dim=0))\n\n                log_softmax = torch.log_softmax(torch.cat(dot_terms, dim=0), dim=0)\n                losses.append(-log_softmax[0,])\n\n            loss = torch.mean(torch.cat(losses))\n            loss.backward()\n\n            sub_batches_processed += img_batch.shape[0]\n            batch_loss += loss.detach().to('cpu')\n            sum_batch_loss += torch.sum(torch.cat(losses).detach().to('cpu'))\n\n            if sub_batches_processed >= args.batch_size:\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n                print(f\"{datetime.datetime.now()} Loss: {batch_loss}\")\n                print(f\"{datetime.datetime.now()} SUM Loss: {sum_batch_loss}\")\n\n                torch.save(res_encoder_model.state_dict(), os.path.join(models_store_path, \"last_res_encoder_weights.pt\"))\n                torch.save(context_predictor_model.state_dict(), os.path.join(models_store_path, \"last_context_predictor_weights.pt\"))\n                \n                if best_batch_loss > batch_loss:\n                    best_batch_loss = batch_loss\n                    best_encoder_model = res_encoder_model #.state_dict()\n                    best_context_model = context_predictor_model #.state_dict()\n                    torch.save(res_encoder_model.state_dict(), os.path.join(models_store_path, best_encoder(prefix)))\n                    torch.save(context_predictor_model.state_dict(), os.path.join(models_store_path, best_context(prefix)))\n\n                sub_batches_processed = 0\n                batch_loss = 0\n                sum_batch_loss = 0\n                \n        # Early stopping\n        if epoch % 5 == 0:\n            valid_loss = run_validation(args, best_encoder_model, best_context_model, random_patch_loader, data_loader_train, data_loader_valid)\n            print('Validation Loss:' +str(valid_loss))\n            \n            if valid_loss > best_valid_loss:\n                trigger += 1\n                if trigger >= patience:\n                    n = epoch - patience\n                    print('Early Stopping! Find best epoch: ' + str(n))\n                    torch.save(real_best_encoder.state_dict(), os.path.join(models_store_path, best_encoder(prefix)))\n                    torch.save(real_best_context.state_dict(), os.path.join(models_store_path, best_context(prefix)))\n                    return\n            else:\n                trigger = 0\n                real_best_encoder = best_encoder_model\n                real_best_context = best_context_model\n                best_valid_loss = valid_loss","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.869001Z","iopub.execute_input":"2023-01-12T18:14:59.869581Z","iopub.status.idle":"2023-01-12T18:14:59.898328Z","shell.execute_reply.started":"2023-01-12T18:14:59.869547Z","shell.execute_reply":"2023-01-12T18:14:59.897379Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# **Anomaly detection evaluation**","metadata":{}},{"cell_type":"code","source":"def calculate_score_dir(enc_patches, predictions, locations, enc_random_patches):\n    losses = []\n\n    for b in range(len(predictions)):\n        \n        p_y = locations[b][0]\n        p_x = locations[b][1]\n\n        target = enc_patches[:,:,p_y,p_x]\n        pred = predictions[b]\n\n        good_term_dot = dot(pred, target)\n        dot_terms = [torch.unsqueeze(good_term_dot,dim=0)]\n\n        for random_patch_idx in range(args.num_random_patches):\n            bad_term_dot = dot(pred, enc_random_patches[random_patch_idx:random_patch_idx+1])\n            dot_terms.append(torch.unsqueeze(bad_term_dot, dim=0))\n\n        log_softmax = torch.log_softmax(torch.cat(dot_terms, dim=0), dim=0)\n        losses.append(-log_softmax[0,])\n\n    loss = torch.mean(torch.cat(losses))\n    return loss\n\ndef run_anomaly_evaluation(args, res_encoder_model_list, context_model_list, models_store_path):\n\n    print(\"RUNNING ANOMALY DETECTION\")\n\n    dataset_train, dataset_valid, dataset_test = get_imagenet_datasets(args.train_image_folder, args.test_image_folder, train_split=1, num_classes_train = args.num_classes_train, num_classes_test = args.num_classes_test)\n    data_loader_test = DataLoader(dataset_test, 1, shuffle = False)\n    NUM_TEST_SAMPLES = dataset_test.get_number_of_samples()\n    print(NUM_TEST_SAMPLES)\n\n    random_patch_loader = get_random_patch_loader(dataset_train) \n    \n    for i, res in enumerate(res_encoder_model_list):\n        res.eval()\n        context_model_list[i].eval()\n    \n    score_all = []\n    label_all = []\n    data_list = []\n    \n    for i, data in enumerate(data_loader_test):\n        data_list.append(data)\n        class_name = data['class_name'][0]\n        \n        img = data['image'].to(args.device)\n        patches = get_patch_tensor_from_image_batch(img)\n        \n        image_scores = []\n        enc_patches = []\n        enc_random_patches = []\n        pred_list = []\n        location_list = []\n        with torch.no_grad():\n            # Get random patches from images not anomalous\n            patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n            if patches_return['is_data_loader_finished']:\n                random_patch_loader = get_random_patch_loader(dataset_train)\n            else:\n                random_patches = patches_return['patches_tensor'].to(args.device)\n                \n            for r, res in enumerate(res_encoder_model_list):\n                # Encode patches of test image\n                temp_patches = res(patches) #49, 1024\n                temp_patches = temp_patches.view(1,7,7,-1) #reshape 1, 7, 7, 1024\n                temp_patches = temp_patches.permute(0,3,1,2) #1, 1024, 7, 7\n                enc_patches.append(temp_patches)\n                \n                # Encode patches of random images\n                temp_random_patches = res(random_patches) #49, 1024\n                enc_random_patches.append(temp_random_patches)\n                \n                # Predictions from encoded patches\n                temp_pred, temp_locations = context_model_list[r](temp_patches) #112, 1024\n                pred_list.append(temp_pred)\n                location_list.append(temp_locations)    \n           \n            print('Image #'+str(i))\n            for j, enc_p in enumerate(enc_patches):\n                score = calculate_score_dir(enc_p, pred_list[j], location_list[j], enc_random_patches[j])\n                print('Score ' +str(j)+ ': ' + str(score.item()))\n                image_scores.append(score)\n            \n            if class_name == 'good':\n                label_all.append(0)\n            else:\n                label_all.append(1)\n            \n            avg_score = sum(image_scores) / len(image_scores) #Media delle loss dei 4 modelli\n            #max_score = torch.max(torch.cat(image_scores)) Loss massima tra i 4 modelli\n            score_all.append(avg_score)\n\n    \n    score_all = [s.cpu().numpy() for s in score_all]\n    score_all = np.vstack(score_all)\n    score_all = np.concatenate(score_all)\n    \n    # Compute threshold -> predictions\n    normal_ratio = sum(1 for a in label_all if a == 0) / len(label_all)\n    threshold = np.percentile(score_all, 100 * normal_ratio)\n    predictions = np.zeros(len(score_all))\n    predictions[score_all > threshold] = 1\n    \n    with open('{0}/Grid30Resnet18.csv'.format(models_store_path), mode='w') as csv_file:\n        fieldnames = ['Class_name', 'Score', 'Anomaly', 'AUC', 'F1', 'Average precision']\n        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n        writer.writeheader()\n        for i, data in enumerate(data_list):\n            print(\"IMAGE #\" + str(i))\n            print(\"Class name: \" + str(data['class_name'][0]))\n            print(\"Average score: \" + str(score_all[i]))\n            print(\"Predictions: \" + str(predictions[i]))\n            \n            writer.writerow({\n                'Class_name': data['class_name'][0],\n                'Score': score_all[i],\n                'Anomaly': predictions[i]})\n    \n        auc = roc_auc_score(label_all, score_all)\n        f1 = compute_pre_recall_f1(label_all, predictions)\n        ap = average_precision_score(label_all, score_all)\n        print('AUC: ' + str(auc) + '\\nF1: ' + str(f1) + '\\nAverage Precision: ' + str(ap))\n        writer.writerow({\n            'AUC': auc,\n            'F1': f1,\n            'Average precision': ap})","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.900628Z","iopub.execute_input":"2023-01-12T18:14:59.901020Z","iopub.status.idle":"2023-01-12T18:14:59.924410Z","shell.execute_reply.started":"2023-01-12T18:14:59.900986Z","shell.execute_reply":"2023-01-12T18:14:59.923391Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN**","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (256,256)\nparser = argparse.ArgumentParser(description='Contrastive predictive coding params')\n\nparser.add_argument('-mode', default='train_cpc' , type=str, help='train_cpc or train_anomaly_detection or anomaly_evaluation')\n# mvtec-ad/bottle/train'          num_class 1         \nparser.add_argument('-train_image_folder', default='../input/mvtec-ad/grid/train', type=str)\nparser.add_argument('-num_classes_train', default=1, type=int)\n# mvtec-ad/bottle/test'                   num_class 4\nparser.add_argument('-test_image_folder', default='../input/mvtec-ad/grid/test', type=str)\nparser.add_argument('-num_classes_test', default=4, type=int)\nparser.add_argument('-batch_size', default=16, type=int)\nparser.add_argument('-sub_batch_size', default=2, type=int)\nparser.add_argument('-num_random_patches', default=15, type=int)\nparser.add_argument('-num_epochs', default=30, type=int)\nparser.add_argument('-patience', default=5, type=int)\n\nparser.add_argument('-layers', default=100, type=int, help='total number of layers (default: 100)')\nparser.add_argument('-growth', default=12, type=int, help='number of new channels per layer (default: 12)')\nparser.add_argument('-reduce', default=0.5, type=float, help='compression rate in transition stage (default: 0.5)')\nparser.add_argument('-no-bottleneck', dest='bottleneck', action='store_false', help='To not use bottleneck block')\nparser.add_argument('-droprate', default=0, type=float, help='dropout probability (default: 0.0)')\n\n# cpu or cuda\nparser.add_argument('-device', default='cuda', type=str)\nparser.add_argument('-model', default='resnet18', type=str, help='There are three implemented models: resnet18, resnet50, densenet')\nparser.add_argument('-resume', default=False, type=bool, help='If we want to start from some weights')\n\nargs, args_other = parser.parse_known_args()\n\nprint(f\"Running CPC with args {args}\")\n\nZ_DIMENSIONS = 1024\nDIRECTIONS = ['DOWN', 'UP', 'RIGHT', 'LEFT']\n\nstored_models_root_path = \"trained_models\"\nif not os.path.isdir(stored_models_root_path):\n    os.mkdir(stored_models_root_path)\nstored_eval_root_path = \"evaluation\"\nif not os.path.isdir(stored_eval_root_path):\n    os.mkdir(stored_eval_root_path)\n    \n# Train the Contrastive Predictive Coding with autoregressive model\nif args.mode == 'train_cpc':\n    \n    # ResNet18 v2 up to the third residual block\n    res_encoder_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device) \n    context_predictor_model = ContextPredictionModelWithAutoregressive(in_channels=Z_DIMENSIONS, direction='down').to(args.device)\n\n    model_store_folder = get_next_model_folder('Context_Pred_Training', stored_models_root_path)\n    os.mkdir(model_store_folder)\n\n    # Model training \n    run_context_predictor(args, res_encoder_model, context_predictor_model, model_store_folder)\n\n# Train anomaly detection model with CPC\nif args.mode == 'train_anomaly_detection':\n    \n    for i, direc in enumerate(DIRECTIONS):\n        res_encoder_model = None\n        context_predictor_model = None\n        \n        # ResNet18 v2 up to the third residual block\n        if args.model == 'resnet18':\n            res_encoder_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device)\n        # ResNet50 v2 up to the third residual block\n        elif args.model == 'resnet50':\n            res_encoder_model = ResNet(3, ResBottleneckBlock, [3, 4, 6, 3], useBottleneck=True).to(args.device)\n        # DenseNet\n        else:\n            res_encoder_model = DenseNet3(args.layers, args.growth, reduction=args.reduce,\n                             bottleneck=args.bottleneck, dropRate=args.droprate).to(args.device)\n        context_predictor_model = ContextPredictionModelWithDir(in_channels=Z_DIMENSIONS, direction=direc).to(args.device)\n        \n        if args.resume:\n            if direc == 'DOWN':\n                res_encoder_weights_path = '/kaggle/input/bottle55densenet/DOWN_best_res_encoder_weights.pt'\n                context_weights_path = '/kaggle/input/bottle55densenet/DOWN_best_context_weights.pt'\n            elif direc == 'UP':\n                res_encoder_weights_path = '/kaggle/input/bottle55densenet/UP_best_res_encoder_weights.pt'\n                context_weights_path = '/kaggle/input/bottle55densenet/UP_best_context_weights.pt'\n            elif direc == 'RIGHT':\n                res_encoder_weights_path = '/kaggle/input/bottle55densenet/RIGHT_best_res_encoder_weights.pt'\n                context_weights_path = '/kaggle/input/bottle55densenet/RIGHT_best_context_weights.pt'\n            else:\n                res_encoder_weights_path = '/kaggle/input/bottle55densenet/LEFT_best_res_encoder_weights.pt'\n                context_weights_path = '/kaggle/input/bottle55densenet/LEFT_best_context_weights.pt'\n        \n            print(f\"Loading res encoder {direc} weights from {res_encoder_weights_path}\")\n            print(f\"Loading context {direc} weights from {context_weights_path}\")\n        \n            # Load weights in the models\n            res_encoder_model.load_state_dict(torch.load(res_encoder_weights_path))\n            context_predictor_model.load_state_dict(torch.load(context_weights_path))\n        \n        # Models training\n        model_store_folder = get_next_model_folder(direc, stored_models_root_path)\n        os.mkdir(model_store_folder)\n        run_context_predictor(args, res_encoder_model, context_predictor_model, model_store_folder)\n\n# Evaluate anomaly detection\nif args.mode == 'anomaly_evaluation':\n    # Evaluation\n    res_encoder_model_list = []\n    context_model_list = []\n    \n    for i, direc in enumerate(DIRECTIONS):\n        res_encoder_model = None\n        context_predictor_model = None\n        res_encoder_weights_path = ''\n        context_weights_path = ''\n        \n        # ResNet18 v2 up to the third residual block\n        if args.model == 'resnet18':\n            res_encoder_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device)\n        # ResNet50 v2 up to the third residual block\n        elif args.model == 'resnet50': \n            res_encoder_model = ResNet(3, ResBottleneckBlock, [3, 4, 6, 3], useBottleneck=True).to(args.device)\n        # DenseNet\n        else:\n            res_encoder_model = DenseNet3(args.layers, args.growth, reduction=args.reduce,\n                             bottleneck=args.bottleneck, dropRate=args.droprate).to(args.device)\n        context_predictor_model = ContextPredictionModelWithDir(in_channels=Z_DIMENSIONS, direction=direc).to(args.device)\n        \n        if direc == 'DOWN':\n            res_encoder_weights_path = '/kaggle/input/gridnew/DOWN_best_res_encoder_weights.pt'\n            context_weights_path = '/kaggle/input/gridnew/DOWN_best_context_weights.pt'\n        elif direc == 'UP':\n            res_encoder_weights_path = '/kaggle/input/gridnew/UP_best_res_encoder_weights.pt'\n            context_weights_path = '/kaggle/input/gridnew/UP_best_context_weights.pt'\n        elif direc == 'RIGHT':\n            res_encoder_weights_path = '/kaggle/input/gridnew/RIGHT_best_res_encoder_weights.pt'\n            context_weights_path = '/kaggle/input/gridnew/RIGHT_best_context_weights.pt'\n        else:\n            res_encoder_weights_path = '/kaggle/input/gridnew/LEFT_best_res_encoder_weights.pt'\n            context_weights_path = '/kaggle/input/gridnew/LEFT_best_context_weights.pt'\n        \n        print(f\"Loading res encoder {direc} weights from {res_encoder_weights_path}\")\n        print(f\"Loading context {direc} weights from {context_weights_path}\")\n        \n        # Load weights in the models\n        res_encoder_model.load_state_dict(torch.load(res_encoder_weights_path))\n        context_predictor_model.load_state_dict(torch.load(context_weights_path))\n        \n        # Encoder models and context models lists\n        res_encoder_model_list.append(res_encoder_model)\n        context_model_list.append(context_predictor_model)\n        \n    run_anomaly_evaluation(args, res_encoder_model_list, context_model_list, stored_eval_root_path)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T18:14:59.926243Z","iopub.execute_input":"2023-01-12T18:14:59.926799Z","iopub.status.idle":"2023-01-12T18:15:27.481107Z","shell.execute_reply.started":"2023-01-12T18:14:59.926759Z","shell.execute_reply":"2023-01-12T18:15:27.479351Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Running CPC with args Namespace(batch_size=16, bottleneck=True, device='cuda', droprate=0, growth=12, layers=100, mode='train_cpc', model='resnet18', num_classes_test=4, num_classes_train=1, num_epochs=30, num_random_patches=15, patience=5, reduce=0.5, resume=False, sub_batch_size=2, test_image_folder='../input/mvtec-ad/grid/test', train_image_folder='../input/mvtec-ad/grid/train')\nSTARTING Context_Pred_Training RUN 3! Storing the models at trained_models/Context_Pred_Training_model_run_3\nRUNNING CONTEXT PREDICTOR down TRAINING\nRUNNING EPOCH #1\n2023-01-12 18:15:12.844503 Loss: 22.893733978271484\n2023-01-12 18:15:12.844633 SUM Loss: 3846.147216796875\n2023-01-12 18:15:25.834969 Loss: 21.88210105895996\n2023-01-12 18:15:25.835129 SUM Loss: 3676.193115234375\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/2397727551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Model training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mrun_context_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_encoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_predictor_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_store_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Train anomaly detection model with CPC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1240976930.py\u001b[0m in \u001b[0;36mrun_context_predictor\u001b[0;34m(args, res_encoder_model, context_predictor_model, models_store_path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mpatches_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_patch_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpatches_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_data_loader_finished'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0mrandom_patch_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_patch_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1251782029.py\u001b[0m in \u001b[0;36mget_random_patches\u001b[0;34m(random_patch_loader, num_random_patches)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mimg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_patch_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mis_data_loader_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1701040066.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1574\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGrayscaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m         \"\"\"\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mrgb_to_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgb_to_grayscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional_pil.py\u001b[0m in \u001b[0;36mto_grayscale\u001b[0;34m(img, num_output_channels)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnum_output_channels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mnp_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mnp_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_img\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    932\u001b[0m         \"\"\"\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0mhas_transparency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transparency\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}