{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!pip install torchsummary\nimport torch\nfrom torch import nn\nfrom torch.nn import Module\nimport datetime\nimport os\nimport argparse\nimport random\nimport csv\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport numpy as np\nimport time\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import roc_auc_score, precision_recall_fscore_support, average_precision_score\nimport torch.nn.functional as F\n#from torchsummary import summary","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-12T20:27:20.637056Z","iopub.execute_input":"2023-01-12T20:27:20.637433Z","iopub.status.idle":"2023-01-12T20:27:20.644460Z","shell.execute_reply.started":"2023-01-12T20:27:20.637401Z","shell.execute_reply":"2023-01-12T20:27:20.643259Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# **Helper functions**","metadata":{}},{"cell_type":"code","source":"def dot_norm_exp(a,b):\n    dot = torch.sum(a * b, dim=1)\n    aa = torch.sum((a**2),dim=1)**0.5\n    bb = torch.sum((b**2),dim=1)**0.5\n    dot_norm = dot/(aa*bb)\n    ret = torch.exp(dot_norm)\n    return ret\n\ndef dot_norm(a,b):\n    dot = torch.sum(a * b, dim=1)\n    aa = torch.sum((a**2),dim=1)**0.5\n    bb = torch.sum((b**2),dim=1)**0.5\n    dot_norm = dot/(aa*bb)\n    return dot_norm\n\ndef dot(a,b):\n    dot = torch.sum(a * b, dim=1)\n    return dot\n\ndef norm_euclidian(a,b):\n    aa = (torch.sum((a**2),dim=1)**0.5).unsqueeze(dim=1)\n    bb = (torch.sum((b**2),dim=1)**0.5).unsqueeze(dim=1)\n    return (torch.sum(((a/aa-b/bb)**2),dim=1)**0.5)\n\ndef get_next_model_folder(prefix, path = ''):\n\n    model_folder = lambda prefix, run_idx: f\"{prefix}_model_run_{run_idx}\"\n\n    run_idx = 1\n    while os.path.isdir(os.path.join(path, model_folder(prefix, run_idx))):\n        run_idx += 1\n\n    model_path = os.path.join(path, model_folder(prefix, run_idx))\n    print(f\"STARTING {prefix} RUN {run_idx}! Storing the models at {model_path}\")\n\n    return model_path\n\ndef get_random_patches(random_patch_loader, num_random_patches):\n\n        is_data_loader_finished = False\n\n        try:\n            img_batch = next(iter(random_patch_loader))['image']\n        except StopIteration:\n            is_data_loader_finished = True\n            # random_patch_loader = DataLoader(dataset_train, num_random_patches, shuffle=True)\n\n        if len(img_batch) < num_random_patches:\n            is_data_loader_finished = True\n\n        patches = []\n\n        for i in range(num_random_patches):\n            x = random.randint(0,6)\n            y = random.randint(0,6)\n\n            patches.append(img_batch[i:i+1,:,x*32:x*32+64,y*32:y*32+64])\n\n            # plt.imshow(np.transpose(patches[-1][0],(1,2,0)))\n            # plt.show()\n\n        patches_tensor = torch.cat(patches, dim=0)\n\n        return dict(\n            patches_tensor = patches_tensor,\n            is_data_loader_finished = is_data_loader_finished)\n\n# Tell how many parameters are on the model\ndef inspect_model(model):\n    param_count = 0\n    for param_tensor_str in model.state_dict():\n        tensor_size = model.state_dict()[param_tensor_str].size()\n        print(f\"{param_tensor_str} size {tensor_size} = {model.state_dict()[param_tensor_str].numel()} params\")\n        param_count += model.state_dict()[param_tensor_str].numel()\n\n    print(f\"Number of parameters: {param_count}\")\n    \n#in img_batch ho due immagini\ndef get_patch_tensor_from_image_batch(img_batch):\n\n    patch_batch = []\n    #here I will put 25 patches of size 256x256\n    all_patches_list_256x256 = []\n    #then from each of the above patches of size 256x256 I will create 49 patches of size 64x64\n    #total will be 25*49=1.225 patches per image of size 64x64\n    all_patches_list = []\n    all_patches_list_temp = []\n\n    for y_patch in range(5):\n        for x_patch in range(5):\n\n            y1 = y_patch * 128\n            y2 = y1 + 256\n\n            x1 = x_patch * 128\n            x2 = x1 + 256\n\n            img_patches = img_batch[:,:,y1:y2,x1:x2] # Batch(img_idx in batch), channels xrange, yrange\n            #img_patches = img_patches.unsqueeze(dim=1)\n            all_patches_list_256x256.append(img_patches)\n\n            # print(patch_batch.shape)\n    i = 0\n    for patch in all_patches_list_256x256:\n        i += 1\n        for y_patch in range(7):\n            for x_patch in range(7):\n\n                y1 = y_patch * 32\n                y2 = y1 + 64\n\n                x1 = x_patch * 32\n                x2 = x1 + 64\n\n                img_patches = patch[:,:,y1:y2,x1:x2] # Batch(img_idx in batch), channels xrange, yrange\n                #print(\"sub patch of big patch number\", i, \" in position\", x_patch, y_patch, \"  x1 = \", x1, \"  x2 = \", x2,  \"  y1 = \",y1, \"  y2 = \", y2)\n                img_patches = img_patches.unsqueeze(dim=1)\n                all_patches_list_temp.append(img_patches)\n        \n        all_patches_list.append(all_patches_list_temp)\n        #print(\"ALL PATCHES LIST SHAPE\", len(all_patches_list))\n        #print(\"ALL PATCHES LIST [0] SHAPE\", len(all_patches_list[0]))\n\n        all_patches_list_temp = []\n            # print(patch_batch.shape)\n    \n    all_patches_tensor_temp = []\n    all_patches_tensor = []\n\n    \n    for i in range(25):\n        all_patches_tensor_temp = torch.cat(all_patches_list[i], dim=1)\n        all_patches_tensor.append(all_patches_tensor_temp)\n\n        \n    patches_per_image = []\n    patches_per_image_temp = []\n    \n    for i in range(25):\n        for b in range(all_patches_tensor[i].shape[0]): #2\n            patches_per_image_temp.append(all_patches_tensor[i][b])\n        patches_per_image.append(patches_per_image_temp)\n        patches_per_image_temp = []\n\n    patch_batch_temp = []\n    for i in range(25):\n        patch_batch_temp = torch.cat(patches_per_image[i], dim = 0)\n        patch_batch.append(patch_batch_temp)\n\n    return patch_batch\n    \ndef write_csv_stats(csv_path, stats_dict):\n\n    if not os.path.isfile(csv_path):\n        with open(csv_path, \"w\") as f:\n            csv_writer = csv.writer(f)\n            csv_writer.writerow(stats_dict.keys())\n\n    for key, value in stats_dict.items():\n        if isinstance(value, float):\n            precision = 0.001\n            stats_dict[key] =  ((value / precision ) // 1.0 ) * precision\n\n    with open(csv_path, \"a\") as f:\n        csv_writer = csv.writer(f)\n        csv_writer.writerow(stats_dict.values())\n\ndef compute_pre_recall_f1(target, pred):\n    precision, recall, f1, _ = precision_recall_fscore_support(target, pred, average='binary')\n    return f1\n\nclass EarlyStopper:\n\n    def stop(self, epoch, val_loss, val_auc=None,  test_loss=None, test_auc=None, test_ap=None,test_f1=None, train_loss=None,score=None,target=None):\n        raise NotImplementedError(\"Implement this method!\")\n\n    def get_best_vl_metrics(self):\n        return  self.train_loss, self.val_loss,self.val_auc,self.test_loss,self.test_auc,self.test_ap,self.test_f1, self.best_epoch,self.score,self.target\n\nclass Patience(EarlyStopper):\n\n    '''\n    Implement common \"patience\" technique\n    '''\n\n    def __init__(self, patience=10, use_train_loss=True):\n        self.local_val_optimum = float(\"inf\")\n        self.use_train_loss = use_train_loss\n        self.patience = patience\n        self.best_epoch = -1\n        self.counter = -1\n\n        self.train_loss= None\n        self.val_loss, self.val_auc, = None, None\n        self.test_loss, self.test_auc,self.test_ap,self.test_f1 = None, None,None, None\n        self.score, self.target = None, None\n\n    def stop(self, epoch, val_loss, val_auc=None, test_loss=None, test_auc=None, test_ap=None,test_f1=None,train_loss=None,score=None,target=None):\n        if self.use_train_loss:\n            if train_loss <= self.local_val_optimum:\n                self.counter = 0\n                self.local_val_optimum = train_loss\n                self.best_epoch = epoch\n                self.train_loss= train_loss\n                self.val_loss, self.val_auc= val_loss, val_auc\n                self.test_loss, self.test_auc, self.test_ap,self.test_f1\\\n                    = test_loss, test_auc, test_ap,test_f1\n                self.score, self.target = score,target\n                return False\n            else:\n                self.counter += 1\n                return self.counter >= self.patience\n        else:\n            if val_loss <= self.local_val_optimum:\n                self.counter = 0\n                self.local_val_optimum = val_loss\n                self.best_epoch = epoch\n                self.train_loss= train_loss\n                self.val_loss, self.val_auc = val_loss, val_auc\n                self.test_loss, self.test_auc, self.test_ap,self.test_f1\\\n                    = test_loss, test_auc, test_ap,test_f1\n                self.score, self.target = score, target\n                return False\n            else:\n                self.counter += 1\n                return self.counter >= self.patience","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:20.764007Z","iopub.execute_input":"2023-01-12T20:27:20.764410Z","iopub.status.idle":"2023-01-12T20:27:20.802871Z","shell.execute_reply.started":"2023-01-12T20:27:20.764376Z","shell.execute_reply":"2023-01-12T20:27:20.801770Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **Dataset**","metadata":{}},{"cell_type":"code","source":"class ImageNetDataset(Dataset):\n    def __init__(self, data_path, is_train, train_split = 0.9, test_split = 1, random_seed = 42, target_transform = None, num_classes = None):\n        super(ImageNetDataset, self).__init__()\n        self.data_path = data_path\n\n        self.is_classes_limited = False\n\n        if num_classes != None:\n            self.is_classes_limited = True\n            self.num_classes = num_classes\n\n        self.classes = []\n        class_idx = 0\n        for class_name in os.listdir(data_path):\n            if not os.path.isdir(os.path.join(data_path,class_name)):\n                continue\n            self.classes.append(\n               dict(\n                   class_idx = class_idx,\n                   class_name = class_name,\n               ))\n            class_idx += 1\n\n            if self.is_classes_limited:\n                if class_idx == self.num_classes:\n                    break\n\n        if not self.is_classes_limited:\n            self.num_classes = len(self.classes)\n\n        self.image_list = []\n        for cls in self.classes:\n            class_path = os.path.join(data_path, cls['class_name'])\n            for image_name in os.listdir(class_path):\n                image_path = os.path.join(class_path, image_name)\n                self.image_list.append(dict(\n                    cls = cls,\n                    image_path = image_path,\n                    image_name = image_name,\n                ))\n\n        self.img_idxes = np.arange(0,len(self.image_list))\n\n        np.random.seed(random_seed)\n\n        if is_train:\n            np.random.shuffle(self.img_idxes)\n            last_train_sample = int(len(self.img_idxes) * train_split)\n            self.img_idxes = self.img_idxes[:last_train_sample]\n            #if is_val:\n            #    self.img_idxes = self.img_idxes[last_train_sample:]\n        else:\n            last_train_sample = int(len(self.img_idxes) * test_split)\n            self.img_idxes = self.img_idxes[last_train_sample:]\n\n    def __len__(self):\n        return len(self.img_idxes)\n\n    def __getitem__(self, index):\n\n        img_idx = self.img_idxes[index]\n        img_info = self.image_list[img_idx]\n\n        img = Image.open(img_info['image_path'])\n        #print('IMG MODE: ' + str(img.mode))\n\n        if img.mode == 'L':\n            tr = transforms.Grayscale(num_output_channels=3)\n            img = tr(img)\n\n        tr = transforms.ToTensor()\n        img1 = tr(img)\n\n        width, height = img.size\n        if min(width, height)>IMG_SIZE[0] * 1.5:\n            tr = transforms.Resize(int(IMG_SIZE[0] * 1.5))\n            img = tr(img)\n\n        width, height = img.size\n        if min(width, height)<IMG_SIZE[0]:\n            tr = transforms.Resize(IMG_SIZE)\n            img = tr(img)\n\n        tr = transforms.RandomCrop(IMG_SIZE)\n        img = tr(img)\n\n        tr = transforms.ToTensor()\n        img = tr(img)\n\n        if (img.shape[0] != 3):\n            img = img[0:3]\n            \n        #plt.imshow(img.permute(1,2,0))\n        #fig, axes = plt.subplots(7,7)\n\n        return dict(image = img, cls = img_info['cls']['class_idx'], class_name = img_info['cls']['class_name'])\n\n    def get_number_of_classes(self):\n        return self.num_classes\n\n    def get_number_of_samples(self):\n        return self.__len__()\n\n    def get_class_names(self):\n        return [cls['class_name'] for cls in self.classes]\n\n    def get_class_name(self, class_idx):\n        return self.classes[class_idx]['class_name']\n\n\ndef get_imagenet_datasets(train_path, test_path, train_split = 0.9, test_split = 1, num_classes_train = None, num_classes_test = None, random_seed = None):\n\n    if random_seed == None:\n        random_seed = int(time.time())\n    dataset_train = ImageNetDataset(train_path, is_train = True, random_seed=random_seed, num_classes = num_classes_train, train_split=train_split)\n    dataset_test = ImageNetDataset(test_path, is_train = False, random_seed=random_seed, num_classes = num_classes_test, train_split=train_split, test_split=test_split)\n\n    return dataset_train, dataset_test\n    \ndef get_random_patch_loader(dataset_train):\n    return DataLoader(dataset_train, args.num_random_patches, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:20.860733Z","iopub.execute_input":"2023-01-12T20:27:20.861376Z","iopub.status.idle":"2023-01-12T20:27:20.882467Z","shell.execute_reply.started":"2023-01-12T20:27:20.861333Z","shell.execute_reply":"2023-01-12T20:27:20.881361Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# **Resnet18-v2 Model**","metadata":{}},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample):\n        super().__init__()\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        if downsample:\n            self.conv1 = nn.Conv2d(\n                in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=2),\n                #nn.MaxPool2d(2,2);\n            )\n        else:\n            self.conv1 = nn.Conv2d(\n                in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            self.shortcut = nn.Sequential()\n        \n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels,\n                               kernel_size=3, stride=1, padding=1)\n        \n\n    def forward(self, input):\n        shortcut = self.shortcut(input)\n        input = self.conv1(nn.ReLU()(self.bn1(input)))\n        input = self.conv2(nn.ReLU()(self.bn2(input)))\n        input = input + shortcut\n        return input","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:20.886484Z","iopub.execute_input":"2023-01-12T20:27:20.886954Z","iopub.status.idle":"2023-01-12T20:27:20.898518Z","shell.execute_reply.started":"2023-01-12T20:27:20.886883Z","shell.execute_reply":"2023-01-12T20:27:20.897399Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, in_channels, resblock, repeat, outputs=1024):\n        super().__init__()\n        self.layer0 = nn.Sequential(\n            nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU()\n        )\n        \n        filters = [64, 64, 128, 256] #, 512]\n\n        self.layer1 = nn.Sequential()\n        self.layer1.add_module('conv2_1', resblock(filters[0], filters[1], downsample=False))\n        for i in range(1, repeat[0]):\n                self.layer1.add_module('conv2_%d'%(i+1,), resblock(filters[1], filters[1], downsample=False))\n\n        self.layer2 = nn.Sequential()\n        self.layer2.add_module('conv3_1', resblock(filters[1], filters[2], downsample=True))\n        for i in range(1, repeat[1]):\n                self.layer2.add_module('conv3_%d' % (\n                    i+1,), resblock(filters[2], filters[2], downsample=False))\n\n        self.layer3 = nn.Sequential()\n        self.layer3.add_module('conv4_1', resblock(filters[2], filters[3], downsample=True))\n        for i in range(1, repeat[2]):\n            self.layer3.add_module('conv2_%d' % (\n                i+1,), resblock(filters[3], filters[3], downsample=False))\n\n        #self.layer4 = nn.Sequential()\n        #self.layer4.add_module('conv5_1', resblock(filters[3], filters[4], downsample=True))\n        #for i in range(1, repeat[3]):\n        #    self.layer4.add_module('conv3_%d'%(i+1,),resblock(filters[4], filters[4], downsample=False))\n\n        self.gap = torch.nn.AdaptiveAvgPool2d(1)\n        self.fc = torch.nn.Linear(filters[3], outputs)\n        \n    def forward(self, input):\n        input = self.layer0(input)\n        input = self.layer1(input)\n        input = self.layer2(input)\n        input = self.layer3(input)\n        #input = self.layer4(input)\n        input = self.gap(input)\n        # torch.flatten()\n        # https://stackoverflow.com/questions/60115633/pytorch-flatten-doesnt-maintain-batch-size\n        input = torch.flatten(input, start_dim=1)\n        input = self.fc(input)\n\n        return input","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:20.910396Z","iopub.execute_input":"2023-01-12T20:27:20.911142Z","iopub.status.idle":"2023-01-12T20:27:20.924237Z","shell.execute_reply.started":"2023-01-12T20:27:20.911104Z","shell.execute_reply":"2023-01-12T20:27:20.923086Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"class ResEncoderModel(Module):\n    def __init__(self):\n        super(ResEncoderModel, self).__init__()\n\n        # Input is 3 x 64 x 64\n        # prep -> 256 x 32 x 32\n\n        self.conv_blocks = [10,10,10] #256x32x32 -> 512x16x16 -> 1024x8x8\n        self.num_blocks = len(self.conv_blocks)\n        self.start_channels = 256\n\n\n        self.prep = nn.Sequential(\n            nn.Conv2d(\n                in_channels = 3,\n                out_channels = self.start_channels,\n                kernel_size = 7,\n                stride = 1, #Let's not reduce twice\n                padding = 3\n            ),\n            nn.BatchNorm2d(\n                num_features = self.start_channels,\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(\n                kernel_size = 3,\n                stride = 2,\n                padding = 1\n            )\n        )\n        # Output 256x 32 x 32\n        current_channels = self.start_channels\n\n        self.resnet_blocks = nn.ModuleList()\n\n        for block_idx, conv_block_num in enumerate(self.conv_blocks):\n            resnet_block = nn.Sequential()\n\n            for conv_block_idx in range(conv_block_num):\n\n                is_downsampling_block = False\n\n                if block_idx > 0 and conv_block_idx == 0:\n                    is_downsampling_block = True\n\n                resnet_block.add_module(\n                    f'conv_{conv_block_idx}',\n                    ResNetBottleneckBlock(\n                        in_channels_block = current_channels,\n                        is_downsampling_block = is_downsampling_block\n                    )\n                )\n\n                if is_downsampling_block:\n                    current_channels *= 2\n\n            self.resnet_blocks.append(resnet_block)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=1)\n\n\n    def forward(self, x):\n\n        x = self.prep.forward(x)\n        #print(f'shape after prep {x.shape}')\n        for i in range(self.num_blocks):\n            x = self.resnet_blocks[i].forward(x)\n            #print(f'shape after resnet_block {i} {x.shape}')\n\n        #print(f'shape after resnet {x.shape}')\n        x = self.avg_pool.forward(x)\n        x = torch.squeeze(x, dim=3)\n        x = torch.squeeze(x, dim=2)\n        ##print(f'shape after avg_pool {x.shape}')\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:20.948382Z","iopub.execute_input":"2023-01-12T20:27:20.948640Z","iopub.status.idle":"2023-01-12T20:27:20.960436Z","shell.execute_reply.started":"2023-01-12T20:27:20.948615Z","shell.execute_reply":"2023-01-12T20:27:20.959548Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# **Context Prediction Model**","metadata":{}},{"cell_type":"markdown","source":"* **CPC with PixelCNN 3x3**","metadata":{}},{"cell_type":"code","source":"class ContextPredictionModel(Module):\n\n    def __init__(self, in_channels):\n        super(ContextPredictionModel, self).__init__()\n\n        self.in_channels = in_channels\n\n        # Input will be 1024x7x7\n\n        # Two sets of convolutional context networks - one for vertical, one for horizontal agregation.\n\n        # Prediction 3 steps ahead. So I will have 8 outputs.\n        # [0,2:6] predict->[3,4,5:6],[1,3:6] predict->[4,5,6:6]\n        # [4,6:6] predict->[3,2,1:6],[3,5:6] predict->[2,1,0:6]\n\n        # [6:0,2] predict->[6:3,4,5],[6:1,3] predict->[6:4,5,6]\n        # [6:4,6] predict->[6:3,2,1],[6:3,5] predict->[6:2,1,0]\n\n        self.context_layers = 3\n        self.context_conv = nn.Sequential()\n\n        for layer_idx in range(self.context_layers):\n            self.context_conv.add_module(f'batch_norm_{layer_idx}',nn.BatchNorm2d(self.in_channels)),\n            self.context_conv.add_module(f'relu_{layer_idx}',nn.ReLU())\n            self.context_conv.add_module(\n                f'conv2d_{layer_idx}',\n                nn.Conv2d(\n                    in_channels = self.in_channels,\n                    out_channels = self.in_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0\n                )\n            )\n\n        self.context_conv.add_module(\n            'adaptive_avg_pool',\n            nn.AdaptiveAvgPool2d(output_size=1)\n        )\n\n\n        # Y direction predictions, X direction predictions\n\n        self.prediction_weights = nn.ModuleList([nn.ModuleList() for i in range(4)])\n\n        # Create linear layers in two directions\n        for direction in range(2):\n            for prediction_steps in range(3):\n                self.prediction_weights[direction].append(\n                    nn.Linear(\n                        in_features = self.in_channels,\n                        out_features = self.in_channels,\n                    )\n                )\n\n\n    # x: encoded patches (2, 1024, 7, 7)\n    def forward(self, x): \n\n        z_patches_list = []\n        z_patches_loc_list = []\n\n        for y1 in range(5):\n            for x1 in range(5):\n                y2 = y1 + 2\n                x2 = x1 + 2\n\n                z_patches = x[:,:,y1:y2+1,x1:x2+1] #2, 1024, 3, 3\n                #print('z_patches: ' + str(z_patches.size()))\n                z_patches_loc = (y1+1,x1+1) # Store middle of the 3x3 square\n\n                z_patches_list.append(z_patches) # itera fino 25\n                #print('List: ' + str(len(z_patches_list)))\n                z_patches_loc_list += [z_patches_loc] * len(z_patches)\n                print('Loc List: ' + str(len(z_patches_loc_list)))\n\n        z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 50, 1024, 3, 3\n        #print('Tensor: ' + str(z_patches_tensor.size()))\n\n        # Apply context model to \n        context_vectors = self.context_conv.forward(z_patches_tensor) #50, 1024, 1, 1\n        #print('Context_vector: ' + str(context_vectors.size()))\n\n        context_vectors = context_vectors.squeeze(dim=3) #50,1024,1\n        #print('Context_vector: ' + str(context_vectors.size()))\n        context_vectors = context_vectors.squeeze(dim=2) #50,1024\n        #print('Context_vector: ' + str(context_vectors.size()))\n\n        context_vectors_for_yp = []\n        context_loc_for_yp = []\n\n        context_vectors_for_xp = []\n        context_loc_for_xp = []\n\n        # Questo gli serve perchÃ¨ lui fa delle predictions in due diverse dimensioni, per cui crea due context vectors\n        # diversi a seconda della direzione\n        for v_idx in range(len(context_vectors)): # 0-49\n            y3 = z_patches_loc_list[v_idx][0]\n            x3 = z_patches_loc_list[v_idx][1]\n\n            if y3 == 1 or y3 == 2:\n                context_vectors_for_yp.append(context_vectors[v_idx:v_idx+1]) #20 finali\n                context_loc_for_yp.append(z_patches_loc_list[v_idx])\n\n            if x3 == 1 or x3 == 2:\n                context_vectors_for_xp.append(context_vectors[v_idx:v_idx+1]) #20 finali\n                context_loc_for_xp.append(z_patches_loc_list[v_idx])\n\n        print('Context_vectors_for_yp: ' + str(len(context_vectors_for_yp)))\n        print('Context_vectors_for_xp: ' + str(len(context_vectors_for_xp)))\n        context_vect_tensor_for_yp = torch.cat(context_vectors_for_yp, dim=0) #20, 1024\n        print('Context_tensor_for_yp: ' + str(context_vect_tensor_for_yp.size())) \n        context_loc_for_yp_t = torch.tensor(context_loc_for_yp) #20\n        print('Context_loc: ' + str(len(context_loc_for_yp_t)))\n\n        context_vect_tensor_for_xp = torch.cat(context_vectors_for_xp, dim=0) #20, 1024\n        print('Context_tensor_for_xp: ' + str(context_vect_tensor_for_xp.size()))\n        context_loc_for_xp_t = torch.tensor(context_loc_for_xp)\n\n        all_predictions = []\n        all_loc = []\n\n        for steps_y in range(3):\n            predictions = self.prediction_weights[0][steps_y].forward(context_vect_tensor_for_yp) #20,1024\n            print('Predictions wrt y: ' + str(predictions.size()))\n            all_predictions.append(predictions)\n            steps_add = torch.tensor([steps_y + 2,0])\n            all_loc.append(context_loc_for_yp_t + steps_add)\n        print('all_loc_y: ' + str(len(all_loc)))\n\n        for steps_x in range(3):\n            predictions = self.prediction_weights[1][steps_x].forward(context_vect_tensor_for_xp)\n            print('Predictions wrt x: ' + str(predictions.size()))\n            all_predictions.append(predictions)\n            steps_add = torch.tensor([0, steps_x + 2])\n            all_loc.append(context_loc_for_xp_t + steps_add)\n\n        ret = torch.cat(all_predictions, dim = 0), torch.cat(all_loc, dim = 0)\n\n        return ret","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:21.049059Z","iopub.execute_input":"2023-01-12T20:27:21.049596Z","iopub.status.idle":"2023-01-12T20:27:21.094965Z","shell.execute_reply.started":"2023-01-12T20:27:21.049565Z","shell.execute_reply":"2023-01-12T20:27:21.093469Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"* **Our version of CPC with PixelCNN**","metadata":{}},{"cell_type":"code","source":"class ContextPredictionModel2(Module):\n\n    def __init__(self, in_channels):\n        super(ContextPredictionModel2, self).__init__()\n\n        self.in_channels = in_channels\n\n        # Input will be 1024x7x7\n\n        # Two sets of convolutional context networks - one for vertical, one for horizontal agregation.\n\n        # Prediction 3 steps ahead. So I will have 8 outputs.\n        # [0,2:6] predict->[3,4,5:6],[1,3:6] predict->[4,5,6:6]\n        # [4,6:6] predict->[3,2,1:6],[3,5:6] predict->[2,1,0:6]\n\n        # [6:0,2] predict->[6:3,4,5],[6:1,3] predict->[6:4,5,6]\n        # [6:4,6] predict->[6:3,2,1],[6:3,5] predict->[6:2,1,0]\n\n        self.context_layers = 3\n        self.context_conv = nn.Sequential()\n\n        for layer_idx in range(self.context_layers):\n            self.context_conv.add_module(f'batch_norm_{layer_idx}',nn.BatchNorm2d(self.in_channels)),\n            self.context_conv.add_module(f'relu_{layer_idx}',nn.ReLU())\n            self.context_conv.add_module(\n                f'conv2d_{layer_idx}',\n                nn.Conv2d(\n                    in_channels = self.in_channels,\n                    out_channels = self.in_channels,\n                    kernel_size=1,\n                    stride=1,\n                    padding=0\n                )\n            )\n\n        self.context_conv.add_module(\n            'adaptive_avg_pool',\n            nn.AdaptiveAvgPool2d(output_size=1)\n        )\n\n\n        # Y direction predictions, X direction predictions\n\n        self.prediction_weights = nn.ModuleList([nn.Linear(\n                        in_features = self.in_channels,\n                        out_features = self.in_channels,\n                    ) for i in range(4)])\n        \n\n    # x: encoded patches (2, 1024, 7, 7)\n    def forward(self, x): \n\n        z_patches_loc_list = []\n        context_vectors_list = []\n\n        for y1 in range(3): #rows\n            z_patches_list = []\n            for x1 in range(7): #columns\n\n                z_patches = x[:,:,0:y1+1,0:7] #2, 1024, o 1 o 2 o 3, 7\n                #print('z_patches: ' + str(z_patches.size()))\n                z_patches_loc = (y1,x1) # Store pixel coordinates\n\n                z_patches_list.append(z_patches) # itera fino 7\n                #print('List: ' + str(len(z_patches_list)))\n                z_patches_loc_list += [z_patches_loc] * len(z_patches)\n                #print('Loc List: ' + str(len(z_patches_loc_list)))\n\n            z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, o 1 o 2 o 3, 3\n            #print('Tensor: ' + str(z_patches_tensor.size()))\n\n            # Apply context model to encoded patches \n            context_vectors_temp = self.context_conv.forward(z_patches_tensor) #14, 1024, 1, 1\n            #print('Context_vector_temp: ' + str(context_vectors_temp.size()))\n            context_vectors_list.append(context_vectors_temp) # 3\n            \n\n        context_vectors = torch.cat(context_vectors_list, dim = 0) #42, 1024, 1, 1\n        context_vectors = context_vectors.squeeze(dim=3)\n        context_vectors = context_vectors.squeeze(dim=2)\n        context_loc_list = torch.tensor(z_patches_loc_list)\n\n\n        all_predictions = []\n        all_loc = []\n\n        for steps_y in range(4):\n            predictions = self.prediction_weights[steps_y].forward(context_vectors) #42, 1024\n            #print('Predictions: ' + str(predictions.size()))\n            all_predictions.append(predictions)\n            steps_add = torch.tensor([steps_y + 1,0])\n            all_loc.append(context_loc_list + steps_add)\n\n        ret = torch.cat(all_predictions, dim = 0), torch.cat(all_loc, dim = 0)\n\n        return ret","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:21.106116Z","iopub.execute_input":"2023-01-12T20:27:21.110430Z","iopub.status.idle":"2023-01-12T20:27:21.132097Z","shell.execute_reply.started":"2023-01-12T20:27:21.110390Z","shell.execute_reply":"2023-01-12T20:27:21.131118Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"* **CPC with only encoder and predicts the underlying patches**","metadata":{}},{"cell_type":"code","source":"class ContextPredictionModelWithDir(Module):\n\n    def __init__(self, in_channels, direction):\n        super(ContextPredictionModelWithDir, self).__init__()\n        # Input will be 1024x7x7\n        self.in_channels = in_channels\n        self.direction = direction\n        \n        self.prediction_weights = nn.ModuleList([nn.Linear(\n                        in_features = self.in_channels,\n                        out_features = self.in_channels,\n                    ) for i in range(4)])\n        \n\n    # x: encoded patches (2, 1024, 7, 7)\n    def forward(self, x): \n\n        z_patches_loc_list = []\n        context_vectors_list = []\n\n        if self.direction == 'DOWN' or self.direction == 'UP':\n            for y1 in range(3): #rows\n                z_patches_list = []\n                for x1 in range(7): #columns\n                    if self.direction == 'DOWN':\n                        z_patches = x[:, :, y1:y1+1, x1:x1+1] #2, 1024, 1, 1\n                        z_patches_loc = (y1,x1) # Store pixel coordinates\n                    else:\n                        z_patches = x[:, :, y1+4:y1+5, x1:x1+1] #2, 1024, 1, 1\n                        z_patches_loc = (y1+4,x1) # Store pixel coordinates\n                    #print('z_patches: ' + str(z_patches.size()))\n                    z_patches_list.append(z_patches) \n                    #print('List: ' + str(len(z_patches_list)))\n                    z_patches_loc_list += [z_patches_loc] * len(z_patches)\n                    #print('Loc List: ' + str(len(z_patches_loc_list)))\n\n                z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, 1, 1\n                #print('Tensor: '+ str(y1) + str(z_patches_tensor.size()))\n                z_patches_list.append(z_patches_tensor) # 3\n        else:\n            for y1 in range(7): #rows\n                z_patches_list = []\n                for x1 in range(3): #columns\n                    if self.direction == 'RIGHT':\n                        z_patches = x[:, :, y1:y1+1, x1:x1+1] #2, 1024, 1, 1\n                        z_patches_loc = (y1,x1) # Store pixel coordinates\n                    else:\n                        z_patches = x[:, :, y1:y1+1, x1+4:x1+5] #2, 1024, 1, 1\n                        z_patches_loc = (y1,x1+4) # Store pixel coordinates\n                    #print('z_patches: ' + str(z_patches.size()))\n                    z_patches_list.append(z_patches) \n                    #print('List: ' + str(len(z_patches_list)))\n                    z_patches_loc_list += [z_patches_loc] * len(z_patches)\n                    #print('Loc List: ' + str(len(z_patches_loc_list)))\n\n                z_patches_tensor = torch.cat(z_patches_list, dim = 0) # 14, 1024, 1, 1\n                #print('Tensor: '+ str(y1) + str(z_patches_tensor.size()))\n                z_patches_list.append(z_patches_tensor) # 3\n            \n        z_patches = torch.cat(z_patches_list, dim = 0) #42, 1024, 1, 1\n        #print('Z_patches_vector: ' + str(z_patches.size()))\n        z_patches = z_patches.squeeze(dim=3)\n        z_patches = z_patches.squeeze(dim=2)\n\n        context_loc_list = torch.tensor(z_patches_loc_list)\n\n        all_predictions = []\n        all_loc = []\n\n        for steps_y in range(4):\n            predictions = self.prediction_weights[steps_y].forward(z_patches) #42, 1024\n            #print('Predictions: ' + str(predictions.size()))\n            all_predictions.append(predictions)\n            if self.direction == 'DOWN':\n                steps_add = torch.tensor([steps_y + 1, 0])\n            elif self.direction == 'UP':\n                steps_add = torch.tensor([0 - 1 - steps_y, 0])\n            elif self.direction == 'RIGHT':\n                steps_add = torch.tensor([0, steps_y + 1])\n            else:\n                steps_add = torch.tensor([0, 0 - 1 - steps_y])\n            all_loc.append(context_loc_list + steps_add)\n\n        ret = torch.cat(all_predictions, dim = 0), torch.cat(all_loc, dim = 0)\n\n        return ret","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:21.196401Z","iopub.execute_input":"2023-01-12T20:27:21.196828Z","iopub.status.idle":"2023-01-12T20:27:21.224922Z","shell.execute_reply.started":"2023-01-12T20:27:21.196792Z","shell.execute_reply":"2023-01-12T20:27:21.223838Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# **Context predictor training**","metadata":{}},{"cell_type":"code","source":"def run_context_predictor(args, res_encoder_model, context_predictor_model, models_store_path):\n\n    print(\"RUNNING CONTEXT PREDICTOR \" +str(context_predictor_model.direction)+ \" TRAINING\")\n    \n    #used to create the file where model weights are saved\n    prefix = str(context_predictor_model.direction)\n    best_encoder = lambda prefix: f\"{prefix}_best_res_encoder_weights.pt\"\n    \n    #upload of datasets\n    dataset_train, dataset_test = get_imagenet_datasets(args.train_image_folder, args.test_image_folder, num_classes_train = args.num_classes_train, num_classes_test = args.num_classes_test)\n    \n    stats_csv_path = os.path.join(models_store_path, \"pred_stats.csv\")\n\n    #creation of dataloaders\n    random_patch_loader = get_random_patch_loader(dataset_train)\n    data_loader_train = DataLoader(dataset_train, args.sub_batch_size, shuffle = True)\n\n    params = list(res_encoder_model.parameters())\n    optimizer = torch.optim.Adam(params = params, lr=0.00001)\n    early_stopper = Patience(100, True)\n\n    z_vect_similarity = dict()\n    \n    for epoch in range(1, args.num_epochs + 1):\n        \n        print(\"RUNNING EPOCH #\" + str(epoch))\n        res_encoder_model.train()\n        \n        for batch in data_loader_train:\n\n            img_batch = batch['image'].to(args.device)\n            patch_batch = get_patch_tensor_from_image_batch(img_batch)\n            #print('Patch_batch: ' + str(patch_batch.size()))\n            batch_size = len(img_batch)\n            \n            sub_batches_processed = 0\n            batch_loss = 0\n            sum_batch_loss = 0 \n            patch_batch_loss = 0\n            best_batch_loss = 1e10\n            patch_sum_batch_loss = 0\n\n            for i in range(len(patch_batch)): #25\n\n                # Apply encoder to all the 49 patches of the image (64x64)\n                patches_encoded = res_encoder_model.forward(patch_batch[i]) #98, 1024\n                #print('Patch_encoded: ' + str(patches_encoded.size()))\n                patches_encoded = patches_encoded.view(batch_size, 7,7,-1) #reshape 2, 7, 7, 1024\n                #print('Reshape: ' + str(patches_encoded.size()))\n                patches_encoded = patches_encoded.permute(0,3,1,2) #2, 1024, 7, 7\n                #print('Permute: ' + str(patches_encoded.size()))\n\n                for i in range(2):\n                    patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n                    if patches_return['is_data_loader_finished']:\n                        random_patch_loader = get_random_patch_loader(dataset_train)\n                    else:\n                        random_patches = patches_return['patches_tensor'].to(args.device)\n\n                # enc_random_patches = resnet_encoder.forward(random_patches).detach()\n                # Apply encoder to few rendom patches\n                enc_random_patches = res_encoder_model.forward(random_patches)\n\n                # Apply context_predictor to encoded patches\n                predictions, locations = context_predictor_model.forward(patches_encoded) #112, 1024\n                #print('Predictions: ' + str(predictions.size())) \n                losses = []\n\n                for b in range(len(predictions)//batch_size): #batch_size = 2\n\n                    b_idx_start = b*batch_size\n                    b_idx_end = (b+1)*batch_size\n\n                    p_y = locations[b_idx_start][0]\n                    p_x = locations[b_idx_start][1]\n\n                    target = patches_encoded[:,:,p_y,p_x]\n                    pred = predictions[b_idx_start:b_idx_end] #2,1024\n\n                    dot_norm_val = dot_norm_exp(pred.detach().to('cpu'), target.detach().to('cpu'))\n                    euc_loss_val = norm_euclidian(pred.detach().to('cpu'), target.detach().to('cpu'))\n\n                    good_term_dot = dot(pred, target)\n                    dot_terms = [torch.unsqueeze(good_term_dot,dim=0)]\n\n                    for random_patch_idx in range(args.num_random_patches):\n\n                        bad_term_dot = dot(pred, enc_random_patches[random_patch_idx:random_patch_idx+1])\n                        dot_terms.append(torch.unsqueeze(bad_term_dot, dim=0))\n\n                    log_softmax = torch.log_softmax(torch.cat(dot_terms, dim=0), dim=0)\n                    losses.append(-log_softmax[0,])\n\n                loss = torch.mean(torch.cat(losses))\n                loss.backward()\n\n                sub_batches_processed += img_batch.shape[0]\n                patch_batch_loss += loss.detach().to('cpu')\n                patch_sum_batch_loss += torch.sum(torch.cat(losses).detach().to('cpu'))\n\n                batch_loss += patch_batch_loss\n                sum_batch_loss += patch_sum_batch_loss\n            \n            batch_loss = batch_loss/25\n\n\n            if sub_batches_processed >= args.batch_size:\n\n                optimizer.step()\n                optimizer.zero_grad()\n\n                print(f\"{datetime.datetime.now()} Loss: {batch_loss}\")\n                print(f\"{datetime.datetime.now()} SUM Loss: {sum_batch_loss}\")\n\n                torch.save(res_encoder_model.state_dict(), os.path.join(models_store_path, \"last_res_encoder_weights.pt\"))\n                #torch.save(context_predictor_model.state_dict(), os.path.join(models_store_path, \"last_context_predictor_weights.pt\"))\n\n                if best_batch_loss > batch_loss:\n                    best_batch_loss = batch_loss\n                    torch.save(res_encoder_model.state_dict(), os.path.join(models_store_path, best_encoder(prefix)))\n                    #torch.save(context_predictor_model.state_dict(), os.path.join(models_store_path, best_encoder(prefix)))\n\n                for key, cos_similarity_tensor in z_vect_similarity.items():\n                    print(f\"Mean cos_sim for class {key} is {cos_similarity_tensor.mean()} . Number: {cos_similarity_tensor.size()}\")\n\n                z_vect_similarity = dict()\n\n                stats = dict(\n                    batch_loss = batch_loss,\n                    sum_batch_loss = sum_batch_loss\n                )\n                write_csv_stats(stats_csv_path, stats)\n\n                sub_batches_processed = 0\n                batch_loss = 0\n                sum_batch_loss = 0","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:21.669872Z","iopub.execute_input":"2023-01-12T20:27:21.670260Z","iopub.status.idle":"2023-01-12T20:27:21.692941Z","shell.execute_reply.started":"2023-01-12T20:27:21.670223Z","shell.execute_reply":"2023-01-12T20:27:21.691962Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"**Anomaly detection evaluation**","metadata":{}},{"cell_type":"code","source":"def calculate_score_dir(enc_patches, predictions, locations, enc_random_patches):\n    losses = []\n\n    for b in range(len(predictions)):\n        #TODO: brutto da sistemare\n        if b < 21:\n            p_y = locations[b][0]\n            p_x = locations[b][1]\n        else:\n            p_y = locations[b - 7][0]\n            p_x = locations[b - 7][1]\n\n        x_t = enc_patches[:,:,p_y,p_x]\n        x_tk = predictions[b]\n        x_tk = x_tk.view(1,1024)\n\n        dot_norm_val = dot_norm_exp(x_tk.detach().to(args.device), x_t.detach().to(args.device))\n        euc_loss_val = norm_euclidian(x_tk.detach().to(args.device), x_t.detach().to(args.device))\n\n        good_term_dot = dot(x_tk, x_t)\n        dot_terms = [torch.unsqueeze(good_term_dot,dim=0)]\n\n        for random_patch_idx in range(args.num_random_patches):\n            bad_term_dot = dot(x_tk, enc_random_patches[random_patch_idx:random_patch_idx+1])\n            dot_terms.append(torch.unsqueeze(bad_term_dot, dim=0))\n\n        log_softmax = torch.log_softmax(torch.cat(dot_terms, dim=0), dim=0)\n        losses.append(-log_softmax[0,])\n        # losses.append(-torch.log(good_term/divisor))\n\n    loss = torch.mean(torch.cat(losses))\n    return loss\n\ndef run_anomaly_evaluation(args, res_encoder_down_model, res_encoder_up_model, res_encoder_right_model, res_encoder_left_model, models_store_path):\n\n    print(\"RUNNING ANOMALY DETECTION\")\n\n    dataset_train, dataset_test = get_imagenet_datasets(args.train_image_folder, args.test_image_folder, train_split=1, test_split=0, num_classes_train = args.num_classes_train, num_classes_test = args.num_classes_test)\n    data_loader_test = DataLoader(dataset_test, 1, shuffle = False)\n    NUM_TEST_SAMPLES = dataset_test.get_number_of_samples()\n    print(NUM_TEST_SAMPLES)\n\n    random_patch_loader = get_random_patch_loader(dataset_train) \n    \n    res_encoder_down_model.eval()\n    res_encoder_up_model.eval()\n    res_encoder_right_model.eval()\n    res_encoder_left_model.eval()\n    \n    score_all = []\n    label_all = []\n    data_list = []\n    \n    for i, data in enumerate(data_loader_test):\n        data_list.append(data)\n        class_name = data['class_name'][0]\n        #print('Class: ' +str(class_name))\n        \n        img = data['image'].to(args.device)\n        patches = get_patch_tensor_from_image_batch(img)\n        #print('Patch_batch: ' + str(patch_batch.size()))\n        #print(data)\n        image_scores = []\n        with torch.no_grad():\n            enc_down_patches = res_encoder_down_model(patches) #49, 1024\n            #print('down size tensor: ' + str(enc_down_patches.size()))\n            enc_down_patches = enc_down_patches.view(1,7,7,-1) #reshape 1, 7, 7, 1024\n            #print('Reshape: ' + str(patches_encoded.size()))\n            enc_down_patches = enc_down_patches.permute(0,3,1,2) #1, 1024, 7, 7\n            #print('Permute: ' + str(patches_encoded.size()))\n            enc_up_patches = res_encoder_up_model(patches) #49, 1024\n            #print('up size tensor: ' + str(enc_up_patches.size()))\n            enc_up_patches = enc_up_patches.view(1,7,7,-1) #reshape 1, 7, 7, 1024\n            #print('Reshape: ' + str(patches_encoded.size()))\n            enc_up_patches = enc_up_patches.permute(0,3,1,2) #1, 1024, 7, 7\n            #print('Permute: ' + str(patches_encoded.size()))\n            enc_right_patches = res_encoder_right_model(patches) #49, 1024\n            #print('right size tensor: ' + str(enc_right_patches.size()))\n            enc_right_patches = enc_right_patches.view(1,7,7,-1) #reshape 1, 7, 7, 1024\n            #print('Reshape: ' + str(patches_encoded.size()))\n            enc_right_patches = enc_right_patches.permute(0,3,1,2) #1, 1024, 7, 7\n            #print('Permute: ' + str(patches_encoded.size()))\n            enc_left_patches = res_encoder_left_model(patches) #49, 1024\n            #print('left size tensor: ' + str(enc_left_patches.size()))\n            enc_left_patches = enc_left_patches.view(1,7,7,-1) #reshape 1, 7, 7, 1024\n            #print('Reshape: ' + str(patches_encoded.size()))\n            enc_left_patches = enc_left_patches.permute(0,3,1,2) #1, 1024, 7, 7\n            #print('Permute: ' + str(patches_encoded.size()))\n            \n            patches_return = get_random_patches(random_patch_loader, args.num_random_patches)\n            if patches_return['is_data_loader_finished']:\n                random_patch_loader = get_random_patch_loader(dataset_train)\n            else:\n                random_patches = patches_return['patches_tensor'].to(args.device)\n            \n            enc_down_random_patches = res_encoder_down_model.forward(random_patches) #49, 1024\n            #print('down size encoded tensor: ' + str(enc_down_patches.size()))\n            enc_up_random_patches = res_encoder_up_model.forward(random_patches) #49, 1024\n            #print('up size encoded tensor: ' + str(enc_up_patches.size()))\n            enc_right_random_patches = res_encoder_right_model.forward(random_patches) #49, 1024\n            #print('right size encoded tensor: ' + str(enc_right_patches.size()))\n            enc_left_random_patches = res_encoder_left_model.forward(random_patches) #49, 1024\n            #print('left size encoded tensor: ' + str(enc_left_patches.size()))\n            \n            predictions_down = enc_down_patches[:,:,3:7,:]  #x_tk\n            predictions_down = predictions_down.permute(0,2,3,1)\n            predictions_down = predictions_down.view(28,1024) # numero totale di predictions (4*7)\n            \n            predictions_up = enc_up_patches[:,:,0:4,:]   #x_tk\n            predictions_up = predictions_up.permute(0,2,3,1)\n            predictions_up = predictions_up.view(28,1024) # numero totale di predictions (4*7)\n            \n            predictions_right = enc_right_patches[:,:,:,3:7]  #x_tk\n            predictions_right = predictions_right.permute(0,2,3,1)\n            predictions_right = predictions_right.reshape(28,1024) # numero totale di predictions (4*7)      \n            \n            predictions_left = enc_left_patches[:,:,:,0:4]  #x_tk\n            predictions_left = predictions_left.permute(0,2,3,1)\n            predictions_left = predictions_left.reshape(28,1024) # numero totale di predictions (4*7)\n            \n            # locations of starting point xt\n            locations_down = [] \n            locations_up = []\n            locations_right = []\n            locations_left = []\n            for x in range(7): #rows\n                for y in range(7): #columns\n                    pos = (x,y)\n                    if x < 3:\n                        locations_down.append(pos)\n                        if y < 3:\n                            locations_right.append(pos)\n                        elif y != 3:\n                            locations_left.append(pos)\n                    elif x != 3:\n                        locations_up.append(pos)\n                        if y < 3:\n                            locations_right.append(pos)\n                        elif y != 3:\n                            locations_left.append(pos)\n                    else:\n                        if y < 3:\n                            locations_right.append(pos)\n                        elif y != 3:\n                            locations_left.append(pos)\n            \n            score_down = calculate_score_dir(enc_down_patches, predictions_down, locations_down, enc_down_random_patches)\n            #print('Down score: ' + str(score_down.item()))\n            image_scores.append(score_down)\n            score_up = calculate_score_dir(enc_up_patches, predictions_up, locations_up, enc_up_random_patches)\n            #print('Up score: ' + str(score_up.item()))\n            image_scores.append(score_up)\n            score_right = calculate_score_dir(enc_right_patches, predictions_right, locations_right, enc_right_random_patches)\n            #print('Right score: ' + str(score_right.item()))\n            image_scores.append(score_right)\n            score_left = calculate_score_dir(enc_left_patches, predictions_left, locations_left, enc_left_random_patches)\n            #print('Left score: ' + str(score_left.item()))\n            image_scores.append(score_left)\n            #print(len(image_scores))\n            \n            if class_name == 'good':\n                label_all.append(0)\n            else:\n                label_all.append(1)\n            \n            avg_score = sum(image_scores) / len(image_scores) #Media delle loss dei 4 modelli\n            #max_score = torch.max(torch.cat(image_scores)) Loss massima tra i 4 modelli\n            score_all.append(avg_score)\n\n    \n    score_all = [s.cpu().numpy() for s in score_all]\n    score_all = np.vstack(score_all)\n    score_all = np.concatenate(score_all)\n    \n    # Compute threshold -> predictions\n    normal_ratio = sum(1 for a in label_all if a == 0) / len(label_all)\n    threshold = np.percentile(score_all, 100 * normal_ratio)\n    predictions = np.zeros(len(score_all))\n    predictions[score_all > threshold] = 1\n    \n    #with open('{0}/Bottle.csv'.format(models_store_path), mode='w') as csv_file:\n    #    fieldnames = ['class_name', 'score', 'anomaly']\n    #    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n    #    writer.writeheader()\n    for i, data in enumerate(data_list):\n        print(\"IMAGE #\" + str(i))\n        print(data['class_name'][0])\n        print(score_all[i])\n        print(predictions[i])\n            \n#      writer.writerow({\n#          'class_name': data['class_name'],\n#          'score': avg_score,\n#          'anomaly': pred_label})\n    \n    auc = roc_auc_score(label_all, score_all)\n    f1 = compute_pre_recall_f1(label_all, predictions)\n    ap = average_precision_score(label_all, score_all)\n    print('AUC: ' + str(auc) + '\\nF1: ' + str(f1) + '\\nAverage Precision: ' + str(ap))\n                     \n","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:21.767651Z","iopub.execute_input":"2023-01-12T20:27:21.767981Z","iopub.status.idle":"2023-01-12T20:27:21.803802Z","shell.execute_reply.started":"2023-01-12T20:27:21.767951Z","shell.execute_reply":"2023-01-12T20:27:21.802752Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# **MAIN**","metadata":{}},{"cell_type":"markdown","source":"* **Anomaly Detection MAIN**\n> Main for detecting anomalies in pictures using Contrastive Predictive Coding","metadata":{}},{"cell_type":"code","source":"IMG_SIZE = (768,768)\nparser = argparse.ArgumentParser(description='Contrastive predictive coding params')\n\n# mode = 'train_encoder_context_prediction'\n# mode = 'anomaly_evaluation'\nparser.add_argument('-mode', default='train_encoder_context_prediction' , type=str)\n# chest-xray-pneumonia/chest_xray/train' \n# mvtec-ad/bottle/train'          num_class 1         \nparser.add_argument('-train_image_folder', default='../input/mvtec-ad/bottle/train', type=str)\nparser.add_argument('-num_classes_train', default=1, type=int)\n# chest-xray-pneumonia/chest_xray/train'  num_class 2\n# mvtec-ad/bottle/test'                   num_class 4\nparser.add_argument('-test_image_folder', default='../input/mvtec-ad/bottle/test', type=str)\nparser.add_argument('-num_classes_test', default=4, type=int)\nparser.add_argument('-batch_size', default=16, type=int)\nparser.add_argument('-sub_batch_size', default=2, type=int)\nparser.add_argument('-num_random_patches', default=15, type=int)\nparser.add_argument('-num_epochs', default=1, type=int)\n# cpu or cuda\nparser.add_argument('-device', default='cuda', type=str)\n\nargs, args_other = parser.parse_known_args()\n\nprint(f\"Running CPC with args {args}\")\n\nZ_DIMENSIONS = 1024\nDIRECTIONS = ['DOWN', 'UP', 'RIGHT', 'LEFT']\n\nstored_models_root_path = \"trained_models\"\nif not os.path.isdir(stored_models_root_path):\n    os.mkdir(stored_models_root_path)\nstored_eval_root_path = \"evaluation\"\nif not os.path.isdir(stored_eval_root_path):\n    os.mkdir(stored_eval_root_path)\n\nif args.mode == 'train_encoder_context_prediction':\n    \n    for i, direc in enumerate(DIRECTIONS):\n        res_encoder_model = None\n        context_predictor_model = None\n        #res_encoder_model = ResEncoderModel().to(args.device)\n        # ResNet18 v2 up to the third residual block\n        res_encoder_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device)\n        context_predictor_model = ContextPredictionModelWithDir(in_channels=Z_DIMENSIONS, direction=direc).to(args.device)\n        \n        # Models training\n        model_store_folder = get_next_model_folder(direc, stored_models_root_path)\n        os.mkdir(model_store_folder)\n        run_context_predictor(args, res_encoder_model, context_predictor_model, model_store_folder)\n    \n    \nif args.mode == 'anomaly_evaluation':\n    # Evaluation\n    res_encoder_down_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device)\n    res_encoder_up_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device)\n    res_encoder_right_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device) \n    res_encoder_left_model = ResNet(3, ResBlock, [2, 2, 2, 2]).to(args.device) \n\n    # Down encoder\n    #models_store_folder = os.path.join(stored_models_root_path, 'Down_model_run_1') #Mettere il numero della run dopo trattino basso\n    #res_encoder_down_weights_path = os.path.join(models_store_folder, \"best_res_encoder_weights.pt\")\n    res_encoder_down_weights_path = '/kaggle/input/models80/down80_best_res_encoder_weights.pt'\n    print(f\"Loading res encoder down weights from {res_encoder_down_weights_path}\")\n    res_encoder_down_model.load_state_dict(torch.load(res_encoder_down_weights_path))\n    \n    # Up encoder\n    #models_store_folder = os.path.join(stored_models_root_path, 'Up_model_run_1') #Mettere il numero della run dopo trattino basso\n    #res_encoder_up_weights_path = os.path.join(models_store_folder, \"best_res_encoder_weights.pt\")\n    res_encoder_up_weights_path = '/kaggle/input/models80/up80_best_res_encoder_weights.pt'\n    print(f\"Loading res encoder up weights from {res_encoder_up_weights_path}\")\n    res_encoder_up_model.load_state_dict(torch.load(res_encoder_up_weights_path))\n    \n    # Right encoder    \n    #models_store_folder = os.path.join(stored_models_root_path, 'Right_model_run_1') #Mettere il numero della run dopo trattino basso\n    #res_encoder_right_weights_path = os.path.join(models_store_folder, \"best_res_encoder_weights.pt\")\n    res_encoder_right_weights_path = '/kaggle/input/models80/right80_best_res_encoder_weights.pt'\n    print(f\"Loading res encoder right weights from {res_encoder_right_weights_path}\")\n    res_encoder_right_model.load_state_dict(torch.load(res_encoder_right_weights_path))\n    \n    # Left encoder\n    #models_store_folder = os.path.join(stored_models_root_path, 'Left_model_run_1') #Mettere il numero della run dopo trattino basso\n    #res_encoder_left_weights_path = os.path.join(models_store_folder, \"best_res_encoder_weights.pt\")\n    res_encoder_left_weights_path = '/kaggle/input/models80/left80_best_res_encoder_weights.pt'\n    print(f\"Loading res encoder left weights from {res_encoder_left_weights_path}\")\n    res_encoder_left_model.load_state_dict(torch.load(res_encoder_left_weights_path))\n    \n    run_anomaly_evaluation(args, res_encoder_down_model, res_encoder_up_model, res_encoder_right_model, res_encoder_left_model, stored_eval_root_path)","metadata":{"execution":{"iopub.status.busy":"2023-01-12T20:27:21.807018Z","iopub.execute_input":"2023-01-12T20:27:21.807826Z","iopub.status.idle":"2023-01-12T20:27:56.747376Z","shell.execute_reply.started":"2023-01-12T20:27:21.807780Z","shell.execute_reply":"2023-01-12T20:27:56.745497Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Running CPC with args Namespace(batch_size=16, device='cuda', mode='train_encoder_context_prediction', num_classes_test=4, num_classes_train=1, num_epochs=1, num_random_patches=15, sub_batch_size=2, test_image_folder='../input/mvtec-ad/bottle/test', train_image_folder='../input/mvtec-ad/bottle/train')\nSTARTING DOWN RUN 3! Storing the models at trained_models/DOWN_model_run_3\nRUNNING CONTEXT PREDICTOR DOWN TRAINING\nRUNNING EPOCH #1\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/336491401.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mmodel_store_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_model_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstored_models_root_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_store_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mrun_context_predictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_encoder_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_predictor_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_store_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2606966662.py\u001b[0m in \u001b[0;36mrun_context_predictor\u001b[0;34m(args, res_encoder_model, context_predictor_model, models_store_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                     \u001b[0mpatches_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_patches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_patch_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_random_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mpatches_return\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_data_loader_finished'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0mrandom_patch_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_random_patch_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/2966360676.py\u001b[0m in \u001b[0;36mget_random_patches\u001b[0;34m(random_patch_loader, num_random_patches)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mimg_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_patch_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mis_data_loader_finished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_23/1991667070.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}